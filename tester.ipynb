{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 16:02:21.499 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Hamed\\AppData\\Local\\miniconda3\\envs\\torch2\\lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2023-06-05 16:02:21.500 `st.experimental_memo` is deprecated. Please use the new command `st.cache_data` instead, which has the same behavior. More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
      "2023-06-05 16:02:21.500 No runtime found, using MemoryCacheStorageManager\n",
      "2023-06-05 16:02:21.502 `st.experimental_memo` is deprecated. Please use the new command `st.cache_data` instead, which has the same behavior. More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
      "2023-06-05 16:02:21.503 No runtime found, using MemoryCacheStorageManager\n",
      "2023-06-05 16:02:21.505 `st.experimental_memo` is deprecated. Please use the new command `st.cache_data` instead, which has the same behavior. More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
      "2023-06-05 16:02:21.506 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "from knowledge_gpt.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 16:02:21.560 No runtime found, using MemoryCacheStorageManager\n"
     ]
    }
   ],
   "source": [
    "pdf = parse_pdf('./knowledge_gpt/file.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = ''.join(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ''.join(text)\n",
    "delimiters = '(\\nAbstract\\n)|(\\n.*Introduction\\n)|(\\n.*Background and Related Work\\n)|(\\n.*Conclusion\\n)|(\\n.*Method\\n)|(\\n.*Related Work\\n)|(\\n.*Evaluation\\n)|(\\n.*Experiments\\n)|(\\n.*Discussion\\n)|(\\n.*Limitations\\n)|(\\n.*References\\n)|(\\n[0-9]\\..*\\n)'\n",
    "text_split = re.split(delimiters, full_text)\n",
    "text = [item for item in text_split if item is not None]\n",
    "text = [item for item in text if len(item)>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Yue Yu* 1Jie Chen* 2 3Tian Gao3Mo Yu3 Abstract Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justiﬁed but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at https:// github.com/fishmoon1234/DAG-GNN . 1. Introduction Bayesian Networks (BN) have been widely used in machine learning applications (Spirtes et al., 1999; Ott et al., 2004). The structure of a BN takes the form of a directed acyclic graph (DAG) and plays a vital part in causal inference (Pearl, *Equal contribution1Lehigh University2MIT-IBM Watson AI Lab3IBM Research. Correspondence to: Yue Yu <yuy214@lehigh.edu >, Jie Chen <chenjie@us.ibm.com >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).1988) with many applications in medicine, genetics, economics, and epidemics. Its structure learning problem is however NP-hard (Chickering et al., 2004) and stimulates a proliferation of literature. Score-based methods generally formulate the structure learning problem as optimizing a certain score function with respect to the unknown (weighted) adjacency matrix Aand the observed data samples, with a combinatorial constraint stating that the graph must be acyclic. The intractable search space (with a complexity superexponential in the number of graph nodes) poses substantial challenges for optimization. Hence, for practical problems in a scale beyond small, approximate search often needs to be employed with additional structure assumption (Nie et al., 2014; Chow & Liu, 1968; Scanagatta et al., 2015; Chen et al., 2016). Recently, Zheng et al. (2018) formulate an equivalent acyclicity constraint by using a continuous function of the adjacency matrix (speciﬁcally, the matrix exponential of A\\x0eA). This approach drastically changes the combinatorial nature of the problem to a continuous optimization, which may be efﬁciently solved by using maturely developed blackbox solvers. The optimization problem is nevertheless nonlinear, thus these solvers generally return only a stationary-point solution rather than the global optimum. Nevertheless, the authors show that empirically such local solutions are highly comparable to the global ones obtained through expensive combinatorial search. With the inspiring reformulation of the constraint, we revisit the objective function. The score-based objective functions generally make assumptions of the variables and the model class. For example, Zheng et al. (2018) demonstrate on the linear structural equation model (SEM) with a leastsquares loss. While convenient, such assumptions are often restricted and they may not correctly reﬂect the actual distribution of real-life data. Hence, motivated by the remarkable success of deep neural networks, which are arguably universal approximators, in this work we develop a graph-based deep generative model aiming at better capturing the sampling distribution faithful to the DAG. To this end, we employ the machinery of variational inference and parameterize a pair of encoder/decoder with specially designed graph neural networks (GNN). The objective function (the score), then, is the evidence lowerarXiv:1904.10098v1  [cs.LG]  22 Apr 2019 bound. Different from the current ﬂourishing designs of GNNs (Bruna et al., 2014; Defferrard et al., 2016; Li et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Chen et al., 2018; Veli ˘ckovi ´c et al., 2018), the proposed ones are generalized from linear SEM, so that the new model performs at least as well as linear SEM when the data is linear. Our proposal has the following distinct features and advantages. First, the work is built on the widespread use of deep generative models (speciﬁcally, variational autoencoders, V AE (Kingma & Welling, 2014)) that are able to capture complex distributions of data and to sample from them. Under the graph setting, the weighted adjacency matrix is an explicit parameter, rather than a latent structure, learnable together with other neural network parameters. The proposed network architecture has not been used before. Second, the framework of V AE naturally handles various data types, notably not only continuous but also discrete ones. All one needs to do is to model the likelihood distribution (decoder output) consistent with the nature of the variables. Third, owing to the use of graph neural networks for parameterization, each variable (node) can be not only scalar-valued but also vector-valued. These variables are considered node features input to/output of the GNNs. Fourth, we propose a variant of the acyclicity constraint more suitable for implementation under current deep learning platforms. The matrix exponential suggested by Zheng et al. (2018), while mathematically elegant, may not be implemented or supported with automatic differentiation in all popular platforms. We propose a polynomial alternative more practically convenient and as numerically stable as the exponential. We demonstrate the effectiveness of the proposed method on synthetic data generated from linear and nonlinear SEMs, benchmark data sets with discrete variables, and data sets from applications. For synthetic data, the proposed DAGGNN outperforms DAG-NOTEARS, the algorithm proposed by Zheng et al. (2018) based on linear SEM. For benchmark data, our learned graphs compare favorably with those obtained through optimizing the Bayesian information criterion by using combinatorial search. 2. Background and Related Work A DAGGand a joint distribution Parefaithful to each other if all and only the conditional independencies true in Pare entailed by G(Pearl, 1988). The faithfulness condition enables one to recover GfromP. Given independent and iid samplesDfrom an unknown distribution corresponding to a faithful but unknown DAG, structure learning refers torecovering the DAG from D. Many exact and approximate algorithms for learning DAG from data have been developed, including score-based and constraint-based approaches (Spirtes et al., 2000a; Chickering, 2002; Koivisto & Sood, 2004; Silander & Myllymaki, 2006; Jaakkola et al., 2010; Cussens, 2011; Yuan & Malone, 2013). Score-based methods generally use a score to measure the goodness of ﬁt of different graphs over data; and then use a search procedure—such as hill-climbing (Heckerman et al., 1995; Tsamardinos et al., 2006; Gmez et al., 2011), forward-backward search (Chickering, 2002), dynamic programming (Singh & Moore, 2005; Silander & Myllymaki, 2006), A\\x03(Yuan & Malone, 2013), or integer programming (Jaakkola et al., 2010; Cussens, 2011; Cussens et al., 2016)—in order to ﬁnd the best graph. Commonly used Bayesian score criteria, such as BDeu and Bayesian information criterion (BIC), are decomposable, consistent, locally consistent (Chickering, 2002), and score equivalent (Heckerman et al., 1995). To make the DAG search space tractable, approximate methods make additional assumptions such as bounded tree-width (Nie et al., 2014), tree-like structures (Chow & Liu, 1968), approximation (Scanagatta et al., 2015), and other constraints about the DAG (Chen et al., 2016). Many bootstrap (Friedman et al., 1999) and sampling-based structure learning algorithms (Madigan et al., 1995; Friedman & Koller, 2003; Eaton & Murphy, 2012; Grzegorczyk & Husmeier, 2008; Niinim ¨aki & Koivisto, 2013; Niinimaki et al., 2012; He et al., 2016) are also proposed to tackle the expensive search problem. Constraint-based methods, in contrast, use (conditional) independence tests to test the existence of edges between each pair of variables. Popular algorithms include SGS (Spirtes et al., 2000b), PC (Spirtes et al., 2000b), IC (Pearl, 2003), and FCI (Spirtes et al., 1995; Zhang, 2008). Recently, there appears a suite of hybrid algorithms that combine score-based and constraint-based methods, such as MMHC (Tsamardinos et al., 2003), and apply constraintbased methods to multiple environments (Mooij et al., 2016). Due to the NP-hardness, traditional DAG learning methods usually deal with discrete variables, as discussed above, or jointly Gaussian variables (Mohan et al., 2012; Mohammadi et al., 2015). Recently, a new continuous optimization approach is proposed (Zheng et al., 2018), which transforms the discrete search procedure into an equality constraint. This approach enables a suite of continuous optimization techniques such as gradient descent to be used. The approach achieves good structure recovery results, although it is applied to only linear SEM for ease of exposition. Neural-network approaches started to surface only very re- cently. Kalainathan et al. (2018) propose a GAN-style (generative adversarial network) method, whereby a separate generative model is applied to each variable and a discriminator is used to distinguish between the joint distributions of real and generated samples. The approach appears to scale well but acyclicity is not enforced. 3. Neural DAG Structure Learning Our method learns the weighted adjacency matrix of a DAG by using a deep generative model that generalizes linear SEM, with which we start the journey. 3.1. Linear Structural Equation Model LetA2Rm\\x02mbe the weighted adjacency matrix of the DAG withmnodes andX2Rm\\x02dbe a sample of a joint distribution of mvariables, where each row corresponds to one variable. In the literature, a variable is typically a scalar, but it can be trivially generalized to a d-dimensional vector under the current setting. The linear SEM model reads X=ATX+Z; (1) whereZ2Rm\\x02dis the noise matrix. When the graph nodes are sorted in the topological order, the matrix Ais strictly upper triangular. Hence, ancestral sampling from the DAG is equivalent to generating a random noise Zfollowed by a triangular solve X= (I\\x00AT)\\x001Z: (2) 3.2. Proposed Graph Neural Network Model Equation (2)may be written as X=fA(Z), a general form recognized by the deep learning community as an abstraction of parameterized graph neural networks that take node features Zas input and return Xas high level representations. Nearly all graph neural networks (Bruna et al., 2014; Defferrard et al., 2016; Li et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Chen et al., 2018; Veli ˘ckovi ´c et al., 2018) can be written in this form. For example, the popular GCN (Kipf & Welling, 2017) architecture reads X=bA\\x01ReLU(bAZW1)\\x01W2; wherebAis a normalization of AandW1andW2are parameter matrices. Owing to the special structure (2), we propose a new graph neural network architecture X=f2((I\\x00AT)\\x001f1(Z)): (3) The parameterized functions f1andf2effectively perform (possibly nonlinear) transforms on ZandX, respectively. If f2is invertible, then (3)is equivalent tof\\x001 2(X) =ATf\\x001 2(X) +f1(Z), a generalized version of the linear SEM (1). We will defer the instantiation of these functions in a later subsection. One of the reasons is that the activation in f2must match the type of the variable X, a subject to be discussed together with discrete variables. 3.3. Model Learning with Variational Autoencoder Given a speciﬁcation of the distribution of Zand samplesX1;:::;Xn, one may learn the generative model (3) through maximizing the log-evidence 1 nnX k=1logp(Xk) =1 nnX k=1logZ p(XkjZ)p(Z)dZ; which, unfortunately, is generally intractable. Hence, we appeal to variational Bayes. To this end, we use a variational posterior q(ZjX)to approximate the actual posterior p(ZjX). The net result is the evidence lower bound (ELBO) LELBO =1 nnX k=1Lk ELBO; with Lk ELBO\\x11\\x00DKL\\x10 q(ZjXk)jjp(Z)\\x11 + Eq(ZjXk)h logp(XkjZ)i :(4) Each individual term Lk ELBO departs from the log-evidence byDKL\\x10 q(ZjXk)jjp(ZjXk)\\x11 \\x150, the KL-divergence between the variational posterior and the actual one. The ELBO lends itself to a variational autoencoder (V AE) (Kingma & Welling, 2014), where given a sample Xk, the encoder (inference model) encodes it into a latent variableZwith density q(ZjXk); and the decoder (generative model) tries to reconstruct XkfromZwith density p(XkjZ). Both densities may be parameterized by using neural networks. Modulo the probability speciﬁcation to be completed later, the generative model (3)discussed in the preceding subsection plays the role of the decoder. Then, we propose the corresponding encoder Z=f4((I\\x00AT)f3(X)); (5) wheref3andf4are parameterized functions that conceptually play the inverse role of f2andf1, respectively. 3.4. Architecture and Loss Function To complete the V AE, one must specify the distributions in(4). Recall that for now both XkandZarem\\x02dmatrices. ENCODERDECODERMLP MLP Figure 1. Architecture (for continuous variables). In the case of discrete variables, the decoder output is changed from MX; SXtoPX. For simplicity, the prior is typically modeled as the standard matrix normal p(Z) =MNm\\x02d(0;I;I). For the inference model, we let f3be a multilayer perceptron (MLP) andf4be the identity mapping. Then, the variational posteriorq(ZjX)is a factored Gaussian with mean MZ2 Rm\\x02dand standard deviation SZ2Rm\\x02d, computed from the encoder [MZjlogSZ] = (I\\x00AT) MLP(X;W1;W2);(6) where MLP(X;W1;W2) := ReLU( XW1)W2, andW1 andW2are parameter matrices. For the generative model, we let f1be the identity mapping andf2be an MLP. Then, the likelihood p(XjZ)is a factored Gaussian with mean MX2Rm\\x02dand standard deviation SX2Rm\\x02d, computed from the decoder [MXjlogSX] = MLP((I\\x00AT)\\x001Z;W3;W4);(7) whereW3andW4are parameter matrices. One may switch the MLP and the identity mapping inside each of the encoder/decoder, but we ﬁnd that the performance is less competitive. One possible reason is that the current design (7)places an emphasis on the nonlinear transform of a sample (I\\x00AT)\\x001Zfrom linear SEM, which better captures nonlinearity. Based on (6)and (7), the KL-divergence term in the ELBO (4) admits a closed form DKL\\x10 q(ZjX)jjp(Z)\\x11 = 1 2mX i=1dX j=1(SZ)2 ij+ (MZ)2 ij\\x002 log(SZ)ij\\x001;(8) and the reconstruction accuracy term may be computed with Monte Carlo approximation Eq(ZjX)h logp(XjZ)i \\x19 1 LLX l=1mX i=1dX j=1\\x00(Xij\\x00(M(l) X)ij)2 2(S(l) X)2 ij\\x00log(S(l) X)ij\\x00c; (9)wherecis a constant and M(l) XandS(l) Xare the outputs of the decoder (7)by taking as input Monte Carlo samples Z(l)\\x18q(ZjX), forl= 1;:::;L . Note that under the autoencoder framework, Zis considered latent (rather than the noise in linear SEM). Hence, the column dimension of Zmay be different from d. From the neural network point of view, changing the column dimension ofZaffects only the sizes of the parameter matrices W2andW3. Sometimes, one may want to use a smaller number than dif he/she observes that the data has a smaller intrinsic dimension. An illustration of the architecture is shown in Figure 1. 3.5. Discrete Variables One advantage of the proposed method is that it naturally handles discrete variables. We assume that each variable has a ﬁnite support of cardinality d. Hence, we let each row of Xbe a one-hot vector, where the “on” location indicates the value of the corresponding variable. We still use standard matrix normal to model the prior and factored Gaussian to model the variational posterior, with (6)being the encoder. On the other hand, we need to slightly modify the likelihood to cope with the discrete nature of the variables. Speciﬁcally, we let p(XjZ)be a factored categorical distribution with probability matrix PX, where each row is a probability vector for the corresponding categorical variable. To achieve so, we change f2from the identity mapping to a row-wise softmax and modify the decoder (7) to PX= softmax(MLP(( I\\x00AT)\\x001Z;W3;W4)):(10) Correspondingly for the ELBO, the KL term (8)remains the same, but the reconstruction term (9)needs be modiﬁed to Eq(ZjX)h logp(XjZ)i \\x191 LLX l=1mX i=1dX j=1Xijlog(P(l) X)ij; (11) whereP(l) Xis the output of the decoder (10) by taking as input Monte Carlo samples Z(l)\\x18q(ZjX), forl= 1;:::;L . 3.6. Connection to Linear SEM One has seen from the forgoing discussions how the proposed model is developed from linear SEM: We apply nonlinearality to the sampling procedure (2)of SEM, treat the resulting generative model as a decoder, and pair with it a variational encoder for tractable learning. Compared with a plain autoencoder, the variational version allows a modeling of the latent space, from which samples are generated. We now proceed, in a reverse thought ﬂow, to establish the connection between the loss function of the linear SEM considered in Zheng et al. (2018) and that of ours. We ﬁrst strip off the variational component of the autoencoder. This plain version uses (5)as the encoder and (3)as the decoder. For notational clarity, let us write bXas the output of the decoder, to distinguish it from the encoder input X. A typical sample loss to minimize is 1 2mX i=1dX j=1(Xij\\x00bXij)2+1 2mX i=1dX j=1Z2 ij; where the ﬁrst term is the reconstruction error and the second term is a regularization of the latent space. One recognizes that the reconstruction error is the same as the negative reconstruction accuracy (9)in the ELBO, up to a constant, if the standard deviation SXis1, the meanMXis taken asbX, and only one Monte Carlo sample is drawn from the variational posterior. Moreover, the regularization term is the same as the KL-divergence (8)in the ELBO if the standard deviation SZis1and the mean MZis taken asZ. If we further strip off the (possibly nonlinear) mappings f1 tof4, then the encoder (5)and decoder (3)read, respectively, Z= (I\\x00AT)XandbX= (I\\x00AT)\\x001Z. This pair results in perfect reconstruction, and hence the sample loss reduces to 1 2mX i=1dX j=1Z2 ij=1 2k(I\\x00AT)Xk2 F; (12) which is the least-squares loss used and justiﬁed by Zheng et al. (2018). 3.7. Acyclicity Constraint Neither maximizing the ELBO (4)nor minimizing the leastsquares loss (12) guarantees that the corresponding graph of the resulting Ais acyclic. Zheng et al. (2018) pair the loss function with an equality constraint, whose satisfaction ensures acyclicity. The idea is based on the fact that the positivity of the (i;j) element of the k-th power of a nonnegative adjacency matrixBindicates the existence of a length- kpath between nodesiandj. Hence, the positivity of the diagonal of Bk reveals cycles. The authors leverage the trick that the matrixexponential admits a Taylor series (because it is analytic on the complex plane), which is nothing but a weighted sum of all nonnegative integer powers of the matrix. The coefﬁcient of the zeorth power (the identity matrix Im\\x02m) is1, and hence the trace of the exponential of Bmust be exactly m for a DAG. To satisfy nonnegativity, one may let Bbe the elementwise square of A; that is,B=A\\x0eA. Whereas the formulation of this acyclicity constraint is mathematically elegant, support of the matrix exponential may not be available in all deep learning platforms. To ease the coding effort, we propose an alternative constraint that is practically convenient. Theorem 1. LetA2Rm\\x02mbe the (possibly negatively) weighted adjacency matrix of a directed graph. For any \\x0b>0, the graph is acyclic if and only if tr[(I+\\x0bA\\x0eA)m]\\x00m= 0: (13) We use (13) as the equality constraint when maximizing the ELBO. The computations of both (I+\\x0bB)mandexp(B) may meet numerical difﬁculty when the eigenvalues of B have a large magnitude. However, the former is less severe than the latter with a judicious choice of \\x0b. Theorem 2. Let\\x0b=c=m> 0for somec. Then for any complex\\x15, we have (1 +\\x0bj\\x15j)m\\x14ecj\\x15j. In practice, \\x0bmay be treated as a hyperparameter and its setting depends on an estimation of the largest eigenvalue of Bin magnitude. This value is the spectral radius of B, and because of nonnegativity, it is bounded by the maximum row sum according to the Perron–Frobenius theorem. 3.8. Training Based on the foregoing, the learning problem is min A;\\x12f(A;\\x12)\\x11\\x00LELBO s.t.h(A)\\x11tr[(I+\\x0bA\\x0eA)m]\\x00m= 0; where the unknowns include the matrix Aand all the parameters \\x12of the V AE (currently we have \\x12= fW1;W2;W3;W4g). Nonlinear equality-constrained problems are well studied and we use the augmented Lagrangian approach to solve it. For completeness, we summarize the algorithm here; the reader is referred to standard textbooks such as Section 4.2 of Bertsekas (1999) for details and convergence analysis. Deﬁne the augmented Lagrangian Lc(A;\\x12;\\x15 ) =f(A;\\x12) +\\x15h(A) +c 2jh(A)j2; where\\x15is the Lagrange multiplier and cis the penalty parameter. When c= +1, the minimizer of Lc(A;\\x12;\\x15 ) must satisfy h(A) = 0 , in which case Lc(A;\\x12;\\x15 )is equal to the objective function f(A;\\x12). Hence, the strategy is to progressively increase c, for each of which minimize the unconstrained augmented Lagrangian. The Lagrange multiplier\\x15is correspondingly updated so that it converges to the one under the optimality condition. There exist a few variants for updating \\x15and increasing c, but a typical effective rule reads: (Ak;\\x12k) = argmin A;\\x12Lck(A;\\x12;\\x15k); (14) \\x15k+1=\\x15k+ckh(Ak); (15) ck+1=( \\x11ck;ifjh(Ak)j>\\rjh(Ak\\x001)j; ck;otherwise;(16) where\\x11>1and\\r <1are tuning parameters. We ﬁnd that often\\x11= 10 and\\r= 1=4work well. The subproblem (14) may be solved by using blackbox stochastic optimization solvers, by noting that the ELBO is deﬁned on a set of samples. 4. Experiments In this section, we present a comprehensive set of experiments to demonstrate the effectiveness of the proposed method DAG-GNN. In Section 4.1, we compare with DAGNOTEARS, the method proposed by Zheng et al. (2018) based on linear SEM, on synthetic data sets generated by sampling generalized linear models, with an emphasis on nonlinear data and vector-valued data ( d > 1). In Section 4.2, we showcase the capability of our model with discrete data, often seen in benchmark data sets with ground truths for assessing quality. To further illustrate the usefulness of the proposed method, in Section 4.3 we apply DAG-GNN on a protein data set for the discovery of consensus protein signaling network, as well as a knowledge base data set for learning causal relations. Our implementation is based on PyTorch (Paszke et al., 2017). We use Adam (Kingma & Ba, 2015) to solve the subproblems (14). To avoid overparameterization, we parameterize the variational posterior q(ZjX)as a factored Gaussian with constant unit variance, and similarly for the likelihoodp(XjZ). When extracting the DAG, we use a thresholding value 0:3, following the recommendation of Zheng et al. (2018). For benchmark and application data sets, we include a Huber-norm regularization of Ain the objective function to encourage more rapid convergence. 4.1. Synthetic Data Sets The synthetic data sets are generated in the following manner. We ﬁrst generate a random DAG by using the Erd ˝os– R´enyi model with expected node degree 3, then assign uni-formly random weights for the edges to obtain the weighted adjacency matrix A. A sampleXis generated by sampling the (generalized) linear model X=g(ATX)+Zwith some functiongelaborated soon. The noise Zfollows standard matrix normal. When the dimension d= 1, we use lowercase letters to denote vectors; that is, x=g(ATx) +z. We compare DAG-GNN with DAG-NOTEARS and report the structural Hamming distance (SHD) and false discovery rate (FDR), each averaged over ﬁve random repetitions. With sample size n= 5000 , we run experiments on four graph sizesm2f10;20;50;100g. In Sections 4.1.1 and 4.1.2 we consider scalar-valued variables ( d= 1) and in Section 4.1.3 vector-valued variables ( d>1). 4.1.1. L INEAR CASE This case is the linear SEM model, with gbeing the identity mapping. The SHD and FDR are plotted in Figure 2. One sees that the graphs learned by the proposed method are substantially more accurate than those by DAG-NOTEARS when the graphs are large. Figure 2. Structure discovery in terms of SHD and FDR to the true graph, on synthetic data set generated by x=ATx+z. 4.1.2. N ONLINEAR CASE We now consider data generated by the following model x=ATh(x) +z; for some nonlinear function h. Taking ﬁrst-order approximationh(x)\\x19h(0)1+h0(0)x(ignoring higher-order terms of x), one obtains an amendatory approximation of the graph adjacency matrix, h0(0)A. This approximate ground truth maintains the DAG structure, with only a scaling on the edge weights. We takeh(x) = cos(x+1)and plot the SHD and FDR in Figure 3. one observes that DAG-GNN slightly improves over DAG-NOTEARS in terms of SHD. Further, FDR is substantially improved, by approximately a factor of three, which indicates that DAG-GNN tends to be more accurate on selecting correct edges. This observation is consistent with the parameter estimates shown in Figure 4, where the ground truth is set as \\x00sin(1)A. The heat map conﬁrms that DAG-GNN results in fewer “false alarms” and recovers a relatively sparser matrix. Figure 3. Structure discovery in terms of SHD and FDR to the true graph, on synthetic data set generated by x=ATcos(x+1) +z. Figure 4. Parameter estimates (before thresholding) of the graph on synthetic data set generated by x=ATcos(x+1) +z. We further experiment with a more complex nonlinear generation model, where the nonlinearity occurs after the linear combination of the variables, as opposed to the preceding case where nonlinearity is applied to the variables before linear combination. Speciﬁcally, we consider x= 2 sin(AT(x+ 0:5\\x011)) +AT(x+ 0:5\\x011) +z; and plot the results in Figure 5. One sees that with higher nonlinearity, the proposed method results in signiﬁcantly better SHD and FDR than does DAG-NOTEARS. Figure 5. Structure discovery in terms of SHD and FDR to the true graph, on synthetic data set generated by x= 2 sin( AT(x+ 0:5\\x01 1)) +AT(x+ 0:5\\x011) +z. 4.1.3. V ECTOR -VALUED CASE The proposed method offers a modeling beneﬁt that the variables can be vector-valued with d>1. Moreover, since Z resides in the latent space of the autoencoder and is not interpreted as noise as in linear SEM, one may take a smaller column dimension dZ<dif he/she believes that the variables have a lower intrinsic dimension. To demonstrate this capability, we construct a data set where the different dimensions come from a randomly scaled and perturbedsample from linear SEM. Speciﬁcally, given a graph adjacency matrix A, we ﬁrst construct a sample ~x2Rm\\x021from the linear SEM ~x=AT~x+ ~z, and then generate for the k-th dimension xk=uk~x+vk+zk, whereukandvkare random scalars from standard normal and zkis a standard normal vector. The eventual sample is X= [x1jx2j\\x01\\x01\\x01jxd]. We letd= 5 anddZ= 1 and compare DAG-GNN with DAG-NOTEARS. The SHD and FDR are plotted in Figure 6. The ﬁgure clearly shows the signiﬁcantly better performance of the proposed method. Moreover, the parameter estimates are shown in Figure 7, compared against the ground-truth A. One sees that the estimated graph from DAG-GNN successfully captures all the ground truth edges and that the estimated weights are also similar. On the other hand, DAG-NOTEARS barely learns the graph. Figure 6. Structure discovery in terms of SHD and FDR to the true graph, on synthetic vector-valued data set. Figure 7. Parameter estimates (before thresholding) of the graph on synthetic vector-valued data set. 4.2. Benchmark Data Sets A beneﬁt of the proposed method is that it naturally handles discrete variables, a case precluded by linear SEM. We demonstrate the use of DAG-GNN on three discrete benchmark data sets: Child, Alarm, and Pigs (Tsamardinos et al., 2006). For comparison is the state-of-the-art exact DAG solver GOPNILP (Cussens et al., 2016), which is based on a constrained integer programming formulation. We use 1000 samples for learning. One sees from Table 1 that our results are reasonably close to the ground truth, whereas not surprisingly the results of GOPNILP are nearly optimal. The BIC score gap exhibits by DAG-GNN may be caused by the relatively simple autoencoder architecture, which is less successful in approximating multinomial distributions. Nevertheless, it is encouraging that the proposed method as a uniﬁed frame- work can handle discrete variables with only slight changes in the network architecture. Table 1. BIC scores on benchmark datasets of discrete variables. Dataset m Groundtruth GOPNILP DAG-GNN Child 20 -1.27e+4 -1.27e+4 -1.38e+4 Alarm 37 -1.07e+4 -1.12e+4 -1.28e+4 Pigs 441 -3.48e+5 -3.50e+5 -3.69e+5 4.3. Applications We consider a bioinformatics data set (Sachs et al., 2005) for the discovery of a protein signaling network based on expression levels of proteins and phospholipids. This is a widely used data set for research on graphical models, with experimental annotations accepted by the biological research community. The data set offers continuous measurements of expression levels of multiple phosphorylated proteins and phospholipid components in human immune system cells, and the modeled network provides the ordering of the connections between pathway components. Based on n= 7466 samples ofm= 11 cell types, Sachs et al. (2005) estimate 20 edges in the graph. In Table 2, we compare DAG-GNN with DAG-NOTEARS as well as FSG, the fast greedy search method proposed by Ramsey et al. (2017), against the ground truth offered by Sachs et al. (2005). The proposed method achieves the lowest SHD. We further show in Figure 8 our estimated graph. One observes that it is acyclic. Our method successfully learns 8 out of 20 ground-truth edges (as marked by red arrows), and predicts 5 indirectly connected edges (blue dashed arrows) as well as 3 reverse edges (yellow arrows). Table 2. Results on protein signaling network: comparison of the predicted graphs with respect to the ground truth. Method SHD # Predicted edges FGS 22 17 NOTEARS 22 16 DAG-GNN 19 18 For another application, we develop a new causal inference task over relations deﬁned in a knowledge base (KB) schema. The task aims at learning a BN, the nodes of which are relations and the edges indicate whether one relation suggests another. For example, the relation person/Nationality may imply person/Language, because the spoken language of a person naturally associates with his/her nationality. This task has a practical value, because most existing KBs are constructed by hand. The success of this task helps suggest meaningful relations for new entities and reduce human efforts. We construct a data set from FB15K-237 (Toutanova et al., 2015) and list in Table 3 a few extracted causal reFigure 8. Estimate protein signaling network. lations. Because of space limitation, we defer the details and more results in the supplementary material. One sees that these results are quite intuitive. We plan a comprehensive study with ﬁeld experts to systematically evaluate the extraction results. Table 3. Examples of extracted edges with high conﬁdence. ﬁlm/ProducedBy ) ﬁlm/Country ﬁlm/ProductionCompanies ) ﬁlm/Country person/Nationality ) person/Languages person/PlaceOfBirth ) person/Languages person/PlaceOfBirth ) person/Nationality person/PlaceLivedLocation ) person/Nationality 5. Conclusion DAG structure learning is a challenging problem that has long been pursued in the literature of graphical models. The difﬁculty, in a large part, is owing to the NP-hardness incurred in the combinatorial formulation. Zheng et al. (2018) propose an equivalent continuous constraint that opens the opportunity of using well developed continuous optimization techniques for solving the problem. In this context, we explore the power of neural networks as functional approximators and develop a deep generative model to capture the complex data distribution, aiming at better recovering the underlying DAG with a different design of the objective function. In particular, we employ the machinery of variational autoencoders and parameterize them with new graph neural network architectures. The proposed method handles not only data generated by parametric models beyond linear, but also variables in general forms, including scalar/vector values and continuous/discrete types. We have performed extensive experiments on synthetic, benchmark, and application data and demonstrated the practical competitiveness of the proposal. References Bertsekas, D. P. Nonlinear Programming . Athena Scientiﬁc, 2nd edition, 1999. Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y . Spectral networks and locally connected networks on graphs. In ICLR , 2014. Chen, E. Y .-J., Shen, Y ., Choi, A., and Darwiche, A. Learning Bayesian networks with ancestral constraints. In NIPS , 2016. Chen, J., Ma, T., and Xiao, C. FastGCN: Fast learning with graph convolutional networks via importance sampling. InICLR , 2018. Chickering, D. M. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning Research , 2002. Chickering, D. M., Heckerman, D., and Meek, C. Largesample learning of Bayesian networks is NP-hard. Journal of Machine Learning Research , 5:1287–1330, 2004. Chow, C. and Liu, C. Approximating discrete probability distributions with dependence trees. IEEE transactions on Information Theory , 14(3):462–467, 1968. Cussens, J. Bayesian network learning with cutting planes. InUAI, 2011. Cussens, J., Haws, D., and Studen `y, M. Polyhedral aspects of score equivalence in Bayesian network structure learning. Mathematical Programming , pp. 1–40, 2016. Defferrard, M., Bresson, X., and Vandergheynst, P. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In NIPS , 2016. Eaton, D. and Murphy, K. Bayesian structure learning using dynamic programming and MCMC. arXiv preprint arXiv:1206.5247 , 2012. Friedman, N. and Koller, D. Being Bayesian about network structure. a Bayesian approach to structure discovery in Bayesian networks. Machine learning , 50(1-2):95–125, 2003. Friedman, N., Goldszmidt, M., and Wyner, A. Data analysis with Bayesian networks: A bootstrap approach. In UAI, 1999. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. In ICML , 2017. Grzegorczyk, M. and Husmeier, D. Improving the structure MCMC sampler for Bayesian networks by introducing a new edge reversal move. Machine Learning , 71(2-3):265, 2008.Gmez, J., Mateo, J., and Puerta, J. Learning Bayesian networks by hill climbing: efﬁcient methods based on progressive restriction of the neighborhood. Data Mining and Knowledge Discovery , 22(1-2):106–148, 2011. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In NIPS , 2017. He, R., Tian, J., and Wu, H. Structure learning in Bayesian networks of a moderate size by efﬁcient sampling. Journal of Machine Learning Research , 17(1):3483–3536, 2016. Heckerman, D., Geiger, D., and Chickering, D. M. Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning , 20(3):197–243, 1995. Jaakkola, T., Sontag, D., Globerson, A., and Meila, M. Learning Bayesian network structure using LP relaxations. 2010. Kalainathan, D., Goudet, O., Guyon, I., Lopez-Paz, D., and Sebag, M. SAM: Structural agnostic model, causal discovery and penalized adversarial learning. arXiv preprint arXiv:1803.04929 , 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR , 2015. Kingma, D. P. and Welling, M. Auto-encoding variational Bayes. In ICLR , 2014. Kipf, T. N. and Welling, M. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR , 2017. Koivisto, M. and Sood, K. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research , 5:549–573, 2004. Li, Y ., Tarlow, D., Brockschmidt, M., and Zemel, R. Gated graph sequence neural networks. In ICLR , 2016. Madigan, D., York, J., and Allard, D. Bayesian graphical models for discrete data. International Statistical Review/Revue Internationale de Statistique , pp. 215–232, 1995. Mohammadi, A., Wit, E. C., et al. Bayesian structure learning in sparse Gaussian graphical models. Bayesian Analysis, 10(1):109–138, 2015. Mohan, K., Chung, M., Han, S., Witten, D., Lee, S.-I., and Fazel, M. Structured learning of Gaussian graphical models. In NIPS , 2012. Mooij, J. M., Magliacane, S., and Claassen, T. Joint causal inference from multiple contexts. arXiv preprint arXiv:1611.10351 , 2016. Nie, S., Mau ´a, D. D., De Campos, C. P., and Ji, Q. Advances in learning Bayesian networks of bounded treewidth. In NIPS , 2014. Niinimaki, T., Parviainen, P., and Koivisto, M. Partial order MCMC for structure discovery in Bayesian networks. arXiv preprint arXiv:1202.3753 , 2012. Niinim ¨aki, T. M. and Koivisto, M. Annealed importance sampling for structure learning in Bayesian networks. In IJCAI , 2013. Ott, S., Imoto, S., and Miyano, S. Finding optimal models for small gene networks. In Paciﬁc symposium on biocomputing , 2004. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in PyTorch. 2017. Pearl, J. Probabilistic reasoning in intelligent systems: networks of plausible inference . Morgan Kaufmann Publishers, Inc., 2 edition, 1988. Pearl, J. Causality: models, reasoning, and inference. Econometric Theory , 19(46):675–685, 2003. Ramsey, J., Glymour, M., Sanchez-Romero, R., and Glymour, C. A million variables and more: the fast greedy equivalence search algorithm for learning highdimensional graphical causal models, with an application to functional magnetic resonance images. International Journal of Data Science and Analytics , 3(2):121–129, 2017. Sachs, K., Perez, O., Peer, D., Lauffenburger, D. A., and Nolan, G. P. Causal protein-signaling networks derived from multiparameter single-cell data. Science , 308(5721): 523–529, 2005. Scanagatta, M., de Campos, C. P., Corani, G., and Zaffalon, M. Learning Bayesian networks with thousands of variables. In NIPS , 2015. Silander, T. and Myllymaki, P. A simple approach for ﬁnding the globally optimal Bayesian network structure. In UAI, 2006. Singh, A. P. and Moore, A. W. Finding optimal Bayesian networks by dynamic programming. Technical report, Carnegie Mellon University, 2005. Spirtes, P., Meek, C., and Richardson, T. Causal inference in the presence of latent variables and selection bias. In UAI, 1995. Spirtes, P., Glymour, C. N., and Scheines, R. Computation, Causation, and Discovery . AAAI Press, 1999.Spirtes, P., Glymour, C., Scheines, R., Kauffman, S., Aimale, V ., and Wimberly, F. Constructing Bayesian network models of gene expression networks from microarray data, 2000a. Spirtes, P., Glymour, C. N., Scheines, R., Heckerman, D., Meek, C., Cooper, G., and Richardson, T. Causation, prediction, and search . MIT press, 2000b. Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., and Gamon, M. Representing text for joint embedding of text and knowledge bases. In EMNLP , 2015. Tsamardinos, I., Aliferis, C. F., and Statnikov, A. Time and sample efﬁcient discovery of markov blankets and direct causal relations. In SIGKDD , 2003. Tsamardinos, I., Brown, L., and Aliferis, C. The maxmin hill-climbing Bayesian network structure learning algorithm. Machine Learning , 65(1):31–78, 2006. Veli˘ckovi ´c, P., Cucurull, G., Casanova, A., Romero, A., Li `o, P., and Bengio, Y . Graph attention networks. In ICLR , 2018. Yuan, C. and Malone, B. Learning optimal Bayesian networks: A shortest path perspective. 48:23–65, 2013. Zhang, J. On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Artiﬁcial Intelligence , 172(16-17): 1873–1896, 2008. Zheng, X., Aragam, B., Ravikumar, P., and Xing, E. P. DAGs with NO TEARS: Continuous optimization for structure learning. In NIPS , 2018. A. Proofs Proof of Theorem 1. LetB=A\\x0eA. Clearly,Bis nonnegative. The binomial expansion reads (I+\\x0bB)m=I+mX k=1\\x12m k\\x13 \\x0bkBk: It is known that there is a cycle of length kif and only if tr(Bk)>0whenB\\x150. Because if there is a cycle then there is a cycle of length at most m, we conclude that there is no cycle if and only if tr[(I+\\x0bB)m] = tr(I) =m. Proof of Theorem 2. Write (1 +\\x0bj\\x15j)m=\\x12 1 +cj\\x15j m\\x13m : For givencandj\\x15j, the right-hand side of the equality is a function of m. This function monotonically increases for positive mand has a limit ecj\\x15j. Hence, for any ﬁnite m> 0,(1 +\\x0bj\\x15j)m\\x14ecj\\x15j. B. Structure Learning over KB Relations We construct the data set from triples in FB15K-237 (Toutanova et al., 2015), which is a subset of FreeBase with approximately 15k entities and 237 relations. Each sample corresponds to an entity and each variable corresponds to a relation in this knowledge base. Each sample has on average 7.36 relations (i.e. 7.36 non-zero entries in each row). Table 4 gives additional examples learned by our model with highest conﬁdence scores. For each target relation on the right-hand side, we show the highest ranked relations within the same domain (i.e. the contents in the ﬁeld before “/” such as “ﬁlm” and “tvProgram”). On the left-hand side, we omit the relations that are common to the associated entity types, e.g. “profession” and “gender” to persons and “genre” to ﬁlms, because almost all entities with these types will contain such a relation. Table 4. (Continued from Table 3) Examples of extracted edges with high conﬁdence. The dot \\x01appearing in R1:R2means that the sample entity is connected to a virtual node (i.e. compound value types in FreeBase) via relation R1, followed by a relation R2to a real entity. ﬁlm/ProducedBy ) ﬁlm/Country ﬁlm/ProductionCompanies ) ﬁlm/Country tvProgram/CountryOfOriginal ) tvProgram/Language tvProgram/RegularCast.regularTv/AppearanceActor ) tvProgram/Language person/Nationality ) person/Languages person/PlaceOfBirth ) person/Languages person/PlaceOfBirth ) person/Nationality person/PlaceLivedLocation ) person/Nationality organization/Headquarters.mailingAddress/Citytown ) organization/PlaceFounded organization/Headquarters.mailingAddress/StateProvince ) organization/PlaceFounded']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Learning a faithful directed acyclic graph (DAG)\\nfrom samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the\\nnumber of graph nodes. A recent breakthrough\\nformulates the problem as a continuous optimization with a structural constraint that ensures\\nacyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation\\nmodel (SEM) and the least-squares loss function\\nthat are statistically well justiﬁed but nevertheless\\nlimited. Motivated by the widespread success of\\ndeep learning that is capable of capturing complex nonlinear mappings, in this work we propose\\na deep generative model and apply a variant of\\nthe structural constraint to learn the DAG. At the\\nheart of the generative model is a variational autoencoder parameterized by a novel graph neural\\nnetwork architecture, which we coin DAG-GNN.\\nIn addition to the richer capacity, an advantage\\nof the proposed model is that it naturally handles discrete variables as well as vector-valued\\nones. We demonstrate that on synthetic data\\nsets, the proposed method learns more accurate\\ngraphs for nonlinearly generated samples; and on\\nbenchmark data sets with discrete variables, the\\nlearned graphs are reasonably close to the global\\noptima. The code is available at https://\\ngithub.com/fishmoon1234/DAG-GNN .', 'Bayesian Networks (BN) have been widely used in machine\\nlearning applications (Spirtes et al., 1999; Ott et al., 2004).\\nThe structure of a BN takes the form of a directed acyclic\\ngraph (DAG) and plays a vital part in causal inference (Pearl,\\n*Equal contribution1Lehigh University2MIT-IBM Watson AI Lab3IBM Research. Correspondence to: Yue Yu\\n<yuy214@lehigh.edu >, Jie Chen <chenjie@us.ibm.com >.\\nProceedings of the 36thInternational Conference on Machine\\nLearning , Long Beach, California, PMLR 97, 2019. Copyright\\n2019 by the author(s).1988) with many applications in medicine, genetics, economics, and epidemics. Its structure learning problem is\\nhowever NP-hard (Chickering et al., 2004) and stimulates a\\nproliferation of literature.\\nScore-based methods generally formulate the structure learning problem as optimizing a certain score function with respect to the unknown (weighted) adjacency matrix Aand\\nthe observed data samples, with a combinatorial constraint\\nstating that the graph must be acyclic. The intractable search\\nspace (with a complexity superexponential in the number\\nof graph nodes) poses substantial challenges for optimization. Hence, for practical problems in a scale beyond small,\\napproximate search often needs to be employed with additional structure assumption (Nie et al., 2014; Chow & Liu,\\n1968; Scanagatta et al., 2015; Chen et al., 2016).\\nRecently, Zheng et al. (2018) formulate an equivalent\\nacyclicity constraint by using a continuous function of the\\nadjacency matrix (speciﬁcally, the matrix exponential of\\nA\\x0eA). This approach drastically changes the combinatorial nature of the problem to a continuous optimization,\\nwhich may be efﬁciently solved by using maturely developed blackbox solvers. The optimization problem is nevertheless nonlinear, thus these solvers generally return only\\na stationary-point solution rather than the global optimum.\\nNevertheless, the authors show that empirically such local\\nsolutions are highly comparable to the global ones obtained\\nthrough expensive combinatorial search.\\nWith the inspiring reformulation of the constraint, we revisit\\nthe objective function. The score-based objective functions\\ngenerally make assumptions of the variables and the model\\nclass. For example, Zheng et al. (2018) demonstrate on\\nthe linear structural equation model (SEM) with a leastsquares loss. While convenient, such assumptions are often\\nrestricted and they may not correctly reﬂect the actual distribution of real-life data.\\nHence, motivated by the remarkable success of deep neural\\nnetworks, which are arguably universal approximators, in\\nthis work we develop a graph-based deep generative model\\naiming at better capturing the sampling distribution faithful\\nto the DAG. To this end, we employ the machinery of variational inference and parameterize a pair of encoder/decoder\\nwith specially designed graph neural networks (GNN). The\\nobjective function (the score), then, is the evidence lowerarXiv:1904.10098v1  [cs.LG]  22 Apr 2019\\nbound. Different from the current ﬂourishing designs of\\nGNNs (Bruna et al., 2014; Defferrard et al., 2016; Li et al.,\\n2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer\\net al., 2017; Chen et al., 2018; Veli ˘ckovi ´c et al., 2018), the\\nproposed ones are generalized from linear SEM, so that the\\nnew model performs at least as well as linear SEM when\\nthe data is linear.\\nOur proposal has the following distinct features and advantages. First, the work is built on the widespread use of deep\\ngenerative models (speciﬁcally, variational autoencoders,\\nV AE (Kingma & Welling, 2014)) that are able to capture\\ncomplex distributions of data and to sample from them. Under the graph setting, the weighted adjacency matrix is an\\nexplicit parameter, rather than a latent structure, learnable together with other neural network parameters. The proposed\\nnetwork architecture has not been used before.\\nSecond, the framework of V AE naturally handles various\\ndata types, notably not only continuous but also discrete\\nones. All one needs to do is to model the likelihood distribution (decoder output) consistent with the nature of the\\nvariables.\\nThird, owing to the use of graph neural networks for parameterization, each variable (node) can be not only scalar-valued\\nbut also vector-valued. These variables are considered node\\nfeatures input to/output of the GNNs.\\nFourth, we propose a variant of the acyclicity constraint\\nmore suitable for implementation under current deep learning platforms. The matrix exponential suggested by Zheng\\net al. (2018), while mathematically elegant, may not be implemented or supported with automatic differentiation in\\nall popular platforms. We propose a polynomial alternative\\nmore practically convenient and as numerically stable as the\\nexponential.\\nWe demonstrate the effectiveness of the proposed method\\non synthetic data generated from linear and nonlinear SEMs,\\nbenchmark data sets with discrete variables, and data sets\\nfrom applications. For synthetic data, the proposed DAGGNN outperforms DAG-NOTEARS, the algorithm proposed by Zheng et al. (2018) based on linear SEM. For\\nbenchmark data, our learned graphs compare favorably with\\nthose obtained through optimizing the Bayesian information\\ncriterion by using combinatorial search.', 'A DAGGand a joint distribution Parefaithful to each other\\nif all and only the conditional independencies true in Pare\\nentailed by G(Pearl, 1988). The faithfulness condition\\nenables one to recover GfromP. Given independent and\\niid samplesDfrom an unknown distribution corresponding\\nto a faithful but unknown DAG, structure learning refers torecovering the DAG from D.\\nMany exact and approximate algorithms for learning DAG\\nfrom data have been developed, including score-based and\\nconstraint-based approaches (Spirtes et al., 2000a; Chickering, 2002; Koivisto & Sood, 2004; Silander & Myllymaki,\\n2006; Jaakkola et al., 2010; Cussens, 2011; Yuan & Malone,\\n2013). Score-based methods generally use a score to measure the goodness of ﬁt of different graphs over data; and\\nthen use a search procedure—such as hill-climbing (Heckerman et al., 1995; Tsamardinos et al., 2006; Gmez et al.,\\n2011), forward-backward search (Chickering, 2002), dynamic programming (Singh & Moore, 2005; Silander &\\nMyllymaki, 2006), A\\x03(Yuan & Malone, 2013), or integer\\nprogramming (Jaakkola et al., 2010; Cussens, 2011; Cussens\\net al., 2016)—in order to ﬁnd the best graph. Commonly\\nused Bayesian score criteria, such as BDeu and Bayesian\\ninformation criterion (BIC), are decomposable, consistent,\\nlocally consistent (Chickering, 2002), and score equivalent (Heckerman et al., 1995).\\nTo make the DAG search space tractable, approximate\\nmethods make additional assumptions such as bounded\\ntree-width (Nie et al., 2014), tree-like structures (Chow\\n& Liu, 1968), approximation (Scanagatta et al., 2015), and\\nother constraints about the DAG (Chen et al., 2016). Many\\nbootstrap (Friedman et al., 1999) and sampling-based structure learning algorithms (Madigan et al., 1995; Friedman\\n& Koller, 2003; Eaton & Murphy, 2012; Grzegorczyk &\\nHusmeier, 2008; Niinim ¨aki & Koivisto, 2013; Niinimaki\\net al., 2012; He et al., 2016) are also proposed to tackle the\\nexpensive search problem.\\nConstraint-based methods, in contrast, use (conditional) independence tests to test the existence of edges between each\\npair of variables. Popular algorithms include SGS (Spirtes\\net al., 2000b), PC (Spirtes et al., 2000b), IC (Pearl,\\n2003), and FCI (Spirtes et al., 1995; Zhang, 2008). Recently, there appears a suite of hybrid algorithms that combine score-based and constraint-based methods, such as\\nMMHC (Tsamardinos et al., 2003), and apply constraintbased methods to multiple environments (Mooij et al.,\\n2016).\\nDue to the NP-hardness, traditional DAG learning methods\\nusually deal with discrete variables, as discussed above, or\\njointly Gaussian variables (Mohan et al., 2012; Mohammadi\\net al., 2015). Recently, a new continuous optimization approach is proposed (Zheng et al., 2018), which transforms\\nthe discrete search procedure into an equality constraint.\\nThis approach enables a suite of continuous optimization\\ntechniques such as gradient descent to be used. The approach achieves good structure recovery results, although it\\nis applied to only linear SEM for ease of exposition.\\nNeural-network approaches started to surface only very re-\\ncently. Kalainathan et al. (2018) propose a GAN-style (generative adversarial network) method, whereby a separate\\ngenerative model is applied to each variable and a discriminator is used to distinguish between the joint distributions of\\nreal and generated samples. The approach appears to scale\\nwell but acyclicity is not enforced.', 'Our method learns the weighted adjacency matrix of a DAG\\nby using a deep generative model that generalizes linear\\nSEM, with which we start the journey.', 'LetA2Rm\\x02mbe the weighted adjacency matrix of the\\nDAG withmnodes andX2Rm\\x02dbe a sample of a joint\\ndistribution of mvariables, where each row corresponds to\\none variable. In the literature, a variable is typically a scalar,\\nbut it can be trivially generalized to a d-dimensional vector\\nunder the current setting. The linear SEM model reads\\nX=ATX+Z; (1)\\nwhereZ2Rm\\x02dis the noise matrix. When the graph\\nnodes are sorted in the topological order, the matrix Ais\\nstrictly upper triangular. Hence, ancestral sampling from the\\nDAG is equivalent to generating a random noise Zfollowed\\nby a triangular solve\\nX= (I\\x00AT)\\x001Z: (2)', 'Equation (2)may be written as X=fA(Z), a general\\nform recognized by the deep learning community as an\\nabstraction of parameterized graph neural networks that\\ntake node features Zas input and return Xas high level\\nrepresentations. Nearly all graph neural networks (Bruna\\net al., 2014; Defferrard et al., 2016; Li et al., 2016; Kipf &\\nWelling, 2017; Hamilton et al., 2017; Gilmer et al., 2017;\\nChen et al., 2018; Veli ˘ckovi ´c et al., 2018) can be written in\\nthis form. For example, the popular GCN (Kipf & Welling,\\n2017) architecture reads\\nX=bA\\x01ReLU(bAZW1)\\x01W2;\\nwherebAis a normalization of AandW1andW2are parameter matrices.\\nOwing to the special structure (2), we propose a new graph\\nneural network architecture\\nX=f2((I\\x00AT)\\x001f1(Z)): (3)\\nThe parameterized functions f1andf2effectively perform (possibly nonlinear) transforms on ZandX, respectively. If f2is invertible, then (3)is equivalent tof\\x001\\n2(X) =ATf\\x001\\n2(X) +f1(Z), a generalized version\\nof the linear SEM (1).\\nWe will defer the instantiation of these functions in a later\\nsubsection. One of the reasons is that the activation in\\nf2must match the type of the variable X, a subject to be\\ndiscussed together with discrete variables.', 'Given a speciﬁcation of the distribution of Zand samplesX1;:::;Xn, one may learn the generative model (3)\\nthrough maximizing the log-evidence\\n1\\nnnX\\nk=1logp(Xk) =1\\nnnX\\nk=1logZ\\np(XkjZ)p(Z)dZ;\\nwhich, unfortunately, is generally intractable. Hence, we\\nappeal to variational Bayes.\\nTo this end, we use a variational posterior q(ZjX)to approximate the actual posterior p(ZjX). The net result is the\\nevidence lower bound (ELBO)\\nLELBO =1\\nnnX\\nk=1Lk\\nELBO;\\nwith\\nLk\\nELBO\\x11\\x00DKL\\x10\\nq(ZjXk)jjp(Z)\\x11\\n+ Eq(ZjXk)h\\nlogp(XkjZ)i\\n:(4)\\nEach individual term Lk\\nELBO departs from the log-evidence\\nbyDKL\\x10\\nq(ZjXk)jjp(ZjXk)\\x11\\n\\x150, the KL-divergence\\nbetween the variational posterior and the actual one.\\nThe ELBO lends itself to a variational autoencoder\\n(V AE) (Kingma & Welling, 2014), where given a sample\\nXk, the encoder (inference model) encodes it into a latent\\nvariableZwith density q(ZjXk); and the decoder (generative model) tries to reconstruct XkfromZwith density\\np(XkjZ). Both densities may be parameterized by using\\nneural networks.\\nModulo the probability speciﬁcation to be completed later,\\nthe generative model (3)discussed in the preceding subsection plays the role of the decoder. Then, we propose the\\ncorresponding encoder\\nZ=f4((I\\x00AT)f3(X)); (5)\\nwheref3andf4are parameterized functions that conceptually play the inverse role of f2andf1, respectively.', 'To complete the V AE, one must specify the distributions\\nin(4). Recall that for now both XkandZarem\\x02dmatrices.\\nENCODERDECODERMLP\\nMLP\\nFigure 1. Architecture (for continuous variables). In the case of discrete variables, the decoder output is changed from MX; SXtoPX.\\nFor simplicity, the prior is typically modeled as the standard\\nmatrix normal p(Z) =MNm\\x02d(0;I;I).\\nFor the inference model, we let f3be a multilayer perceptron\\n(MLP) andf4be the identity mapping. Then, the variational\\nposteriorq(ZjX)is a factored Gaussian with mean MZ2\\nRm\\x02dand standard deviation SZ2Rm\\x02d, computed from\\nthe encoder\\n[MZjlogSZ] = (I\\x00AT) MLP(X;W1;W2);(6)\\nwhere MLP(X;W1;W2) := ReLU( XW1)W2, andW1\\nandW2are parameter matrices.\\nFor the generative model, we let f1be the identity mapping\\nandf2be an MLP. Then, the likelihood p(XjZ)is a factored\\nGaussian with mean MX2Rm\\x02dand standard deviation\\nSX2Rm\\x02d, computed from the decoder\\n[MXjlogSX] = MLP((I\\x00AT)\\x001Z;W3;W4);(7)\\nwhereW3andW4are parameter matrices.\\nOne may switch the MLP and the identity mapping inside\\neach of the encoder/decoder, but we ﬁnd that the performance is less competitive. One possible reason is that the\\ncurrent design (7)places an emphasis on the nonlinear transform of a sample (I\\x00AT)\\x001Zfrom linear SEM, which\\nbetter captures nonlinearity.\\nBased on (6)and (7), the KL-divergence term in the\\nELBO (4) admits a closed form\\nDKL\\x10\\nq(ZjX)jjp(Z)\\x11\\n=\\n1\\n2mX\\ni=1dX\\nj=1(SZ)2\\nij+ (MZ)2\\nij\\x002 log(SZ)ij\\x001;(8)\\nand the reconstruction accuracy term may be computed with\\nMonte Carlo approximation\\nEq(ZjX)h\\nlogp(XjZ)i\\n\\x19\\n1\\nLLX\\nl=1mX\\ni=1dX\\nj=1\\x00(Xij\\x00(M(l)\\nX)ij)2\\n2(S(l)\\nX)2\\nij\\x00log(S(l)\\nX)ij\\x00c;\\n(9)wherecis a constant and M(l)\\nXandS(l)\\nXare the outputs of\\nthe decoder (7)by taking as input Monte Carlo samples\\nZ(l)\\x18q(ZjX), forl= 1;:::;L .\\nNote that under the autoencoder framework, Zis considered latent (rather than the noise in linear SEM). Hence, the\\ncolumn dimension of Zmay be different from d. From the\\nneural network point of view, changing the column dimension ofZaffects only the sizes of the parameter matrices\\nW2andW3. Sometimes, one may want to use a smaller\\nnumber than dif he/she observes that the data has a smaller\\nintrinsic dimension.\\nAn illustration of the architecture is shown in Figure 1.', 'One advantage of the proposed method is that it naturally\\nhandles discrete variables. We assume that each variable\\nhas a ﬁnite support of cardinality d.\\nHence, we let each row of Xbe a one-hot vector, where\\nthe “on” location indicates the value of the corresponding\\nvariable. We still use standard matrix normal to model\\nthe prior and factored Gaussian to model the variational\\nposterior, with (6)being the encoder. On the other hand,\\nwe need to slightly modify the likelihood to cope with the\\ndiscrete nature of the variables.\\nSpeciﬁcally, we let p(XjZ)be a factored categorical distribution with probability matrix PX, where each row is a\\nprobability vector for the corresponding categorical variable.\\nTo achieve so, we change f2from the identity mapping to a\\nrow-wise softmax and modify the decoder (7) to\\nPX= softmax(MLP(( I\\x00AT)\\x001Z;W3;W4)):(10)\\nCorrespondingly for the ELBO, the KL term (8)remains\\nthe same, but the reconstruction term (9)needs be modiﬁed\\nto\\nEq(ZjX)h\\nlogp(XjZ)i\\n\\x191\\nLLX\\nl=1mX\\ni=1dX\\nj=1Xijlog(P(l)\\nX)ij;\\n(11)\\nwhereP(l)\\nXis the output of the decoder (10) by taking as input Monte Carlo samples Z(l)\\x18q(ZjX), forl= 1;:::;L .', 'One has seen from the forgoing discussions how the proposed model is developed from linear SEM: We apply nonlinearality to the sampling procedure (2)of SEM, treat the\\nresulting generative model as a decoder, and pair with it a\\nvariational encoder for tractable learning. Compared with a\\nplain autoencoder, the variational version allows a modeling\\nof the latent space, from which samples are generated.\\nWe now proceed, in a reverse thought ﬂow, to establish the\\nconnection between the loss function of the linear SEM\\nconsidered in Zheng et al. (2018) and that of ours. We\\nﬁrst strip off the variational component of the autoencoder.\\nThis plain version uses (5)as the encoder and (3)as the\\ndecoder. For notational clarity, let us write bXas the output\\nof the decoder, to distinguish it from the encoder input X.\\nA typical sample loss to minimize is\\n1\\n2mX\\ni=1dX\\nj=1(Xij\\x00bXij)2+1\\n2mX\\ni=1dX\\nj=1Z2\\nij;\\nwhere the ﬁrst term is the reconstruction error and the second\\nterm is a regularization of the latent space. One recognizes\\nthat the reconstruction error is the same as the negative\\nreconstruction accuracy (9)in the ELBO, up to a constant,\\nif the standard deviation SXis1, the meanMXis taken\\nasbX, and only one Monte Carlo sample is drawn from\\nthe variational posterior. Moreover, the regularization term\\nis the same as the KL-divergence (8)in the ELBO if the\\nstandard deviation SZis1and the mean MZis taken asZ.\\nIf we further strip off the (possibly nonlinear) mappings f1\\ntof4, then the encoder (5)and decoder (3)read, respectively,\\nZ= (I\\x00AT)XandbX= (I\\x00AT)\\x001Z. This pair results\\nin perfect reconstruction, and hence the sample loss reduces\\nto\\n1\\n2mX\\ni=1dX\\nj=1Z2\\nij=1\\n2k(I\\x00AT)Xk2\\nF; (12)\\nwhich is the least-squares loss used and justiﬁed by Zheng\\net al. (2018).', 'Neither maximizing the ELBO (4)nor minimizing the leastsquares loss (12) guarantees that the corresponding graph\\nof the resulting Ais acyclic. Zheng et al. (2018) pair the\\nloss function with an equality constraint, whose satisfaction\\nensures acyclicity.\\nThe idea is based on the fact that the positivity of the (i;j)\\nelement of the k-th power of a nonnegative adjacency matrixBindicates the existence of a length- kpath between\\nnodesiandj. Hence, the positivity of the diagonal of Bk\\nreveals cycles. The authors leverage the trick that the matrixexponential admits a Taylor series (because it is analytic on\\nthe complex plane), which is nothing but a weighted sum of\\nall nonnegative integer powers of the matrix. The coefﬁcient\\nof the zeorth power (the identity matrix Im\\x02m) is1, and\\nhence the trace of the exponential of Bmust be exactly m\\nfor a DAG. To satisfy nonnegativity, one may let Bbe the\\nelementwise square of A; that is,B=A\\x0eA.\\nWhereas the formulation of this acyclicity constraint is mathematically elegant, support of the matrix exponential may\\nnot be available in all deep learning platforms. To ease the\\ncoding effort, we propose an alternative constraint that is\\npractically convenient.\\nTheorem 1. LetA2Rm\\x02mbe the (possibly negatively)\\nweighted adjacency matrix of a directed graph. For any\\n\\x0b>0, the graph is acyclic if and only if\\ntr[(I+\\x0bA\\x0eA)m]\\x00m= 0: (13)\\nWe use (13) as the equality constraint when maximizing the\\nELBO. The computations of both (I+\\x0bB)mandexp(B)\\nmay meet numerical difﬁculty when the eigenvalues of B\\nhave a large magnitude. However, the former is less severe\\nthan the latter with a judicious choice of \\x0b.\\nTheorem 2. Let\\x0b=c=m> 0for somec. Then for any\\ncomplex\\x15, we have (1 +\\x0bj\\x15j)m\\x14ecj\\x15j.\\nIn practice, \\x0bmay be treated as a hyperparameter and its\\nsetting depends on an estimation of the largest eigenvalue of\\nBin magnitude. This value is the spectral radius of B, and\\nbecause of nonnegativity, it is bounded by the maximum\\nrow sum according to the Perron–Frobenius theorem.', 'Based on the foregoing, the learning problem is\\nmin\\nA;\\x12f(A;\\x12)\\x11\\x00LELBO\\ns.t.h(A)\\x11tr[(I+\\x0bA\\x0eA)m]\\x00m= 0;\\nwhere the unknowns include the matrix Aand all the\\nparameters \\x12of the V AE (currently we have \\x12=\\nfW1;W2;W3;W4g). Nonlinear equality-constrained\\nproblems are well studied and we use the augmented Lagrangian approach to solve it. For completeness, we summarize the algorithm here; the reader is referred to standard\\ntextbooks such as Section 4.2 of Bertsekas (1999) for details\\nand convergence analysis.\\nDeﬁne the augmented Lagrangian\\nLc(A;\\x12;\\x15 ) =f(A;\\x12) +\\x15h(A) +c\\n2jh(A)j2;\\nwhere\\x15is the Lagrange multiplier and cis the penalty\\nparameter. When c= +1, the minimizer of Lc(A;\\x12;\\x15 )\\nmust satisfy h(A) = 0 , in which case Lc(A;\\x12;\\x15 )is equal\\nto the objective function f(A;\\x12). Hence, the strategy is\\nto progressively increase c, for each of which minimize\\nthe unconstrained augmented Lagrangian. The Lagrange\\nmultiplier\\x15is correspondingly updated so that it converges\\nto the one under the optimality condition.\\nThere exist a few variants for updating \\x15and increasing c,\\nbut a typical effective rule reads:\\n(Ak;\\x12k) = argmin\\nA;\\x12Lck(A;\\x12;\\x15k); (14)\\n\\x15k+1=\\x15k+ckh(Ak); (15)\\nck+1=(\\n\\x11ck;ifjh(Ak)j>\\rjh(Ak\\x001)j;\\nck;otherwise;(16)\\nwhere\\x11>1and\\r <1are tuning parameters. We ﬁnd that\\noften\\x11= 10 and\\r= 1=4work well.\\nThe subproblem (14) may be solved by using blackbox\\nstochastic optimization solvers, by noting that the ELBO is\\ndeﬁned on a set of samples.', 'In this section, we present a comprehensive set of experiments to demonstrate the effectiveness of the proposed\\nmethod DAG-GNN. In Section 4.1, we compare with DAGNOTEARS, the method proposed by Zheng et al. (2018)\\nbased on linear SEM, on synthetic data sets generated by\\nsampling generalized linear models, with an emphasis on\\nnonlinear data and vector-valued data ( d > 1). In Section 4.2, we showcase the capability of our model with\\ndiscrete data, often seen in benchmark data sets with ground\\ntruths for assessing quality. To further illustrate the usefulness of the proposed method, in Section 4.3 we apply\\nDAG-GNN on a protein data set for the discovery of consensus protein signaling network, as well as a knowledge\\nbase data set for learning causal relations.\\nOur implementation is based on PyTorch (Paszke et al.,\\n2017). We use Adam (Kingma & Ba, 2015) to solve the\\nsubproblems (14). To avoid overparameterization, we parameterize the variational posterior q(ZjX)as a factored\\nGaussian with constant unit variance, and similarly for the\\nlikelihoodp(XjZ). When extracting the DAG, we use\\na thresholding value 0:3, following the recommendation\\nof Zheng et al. (2018). For benchmark and application data\\nsets, we include a Huber-norm regularization of Ain the\\nobjective function to encourage more rapid convergence.', 'The synthetic data sets are generated in the following manner. We ﬁrst generate a random DAG by using the Erd ˝os–\\nR´enyi model with expected node degree 3, then assign uni-formly random weights for the edges to obtain the weighted\\nadjacency matrix A. A sampleXis generated by sampling\\nthe (generalized) linear model X=g(ATX)+Zwith some\\nfunctiongelaborated soon. The noise Zfollows standard\\nmatrix normal. When the dimension d= 1, we use lowercase letters to denote vectors; that is, x=g(ATx) +z. We\\ncompare DAG-GNN with DAG-NOTEARS and report the\\nstructural Hamming distance (SHD) and false discovery rate\\n(FDR), each averaged over ﬁve random repetitions. With\\nsample size n= 5000 , we run experiments on four graph\\nsizesm2f10;20;50;100g. In Sections 4.1.1 and 4.1.2\\nwe consider scalar-valued variables ( d= 1) and in Section 4.1.3 vector-valued variables ( d>1).', 'This case is the linear SEM model, with gbeing the identity\\nmapping. The SHD and FDR are plotted in Figure 2. One\\nsees that the graphs learned by the proposed method are\\nsubstantially more accurate than those by DAG-NOTEARS\\nwhen the graphs are large.\\nFigure 2. Structure discovery in terms of SHD and FDR to the true\\ngraph, on synthetic data set generated by x=ATx+z.', 'We now consider data generated by the following model\\nx=ATh(x) +z;\\nfor some nonlinear function h. Taking ﬁrst-order approximationh(x)\\x19h(0)1+h0(0)x(ignoring higher-order terms of\\nx), one obtains an amendatory approximation of the graph\\nadjacency matrix, h0(0)A. This approximate ground truth\\nmaintains the DAG structure, with only a scaling on the\\nedge weights.\\nWe takeh(x) = cos(x+1)and plot the SHD and FDR in\\nFigure 3. one observes that DAG-GNN slightly improves\\nover DAG-NOTEARS in terms of SHD. Further, FDR is\\nsubstantially improved, by approximately a factor of three,\\nwhich indicates that DAG-GNN tends to be more accurate\\non selecting correct edges. This observation is consistent\\nwith the parameter estimates shown in Figure 4, where the\\nground truth is set as \\x00sin(1)A. The heat map conﬁrms\\nthat DAG-GNN results in fewer “false alarms” and recovers\\na relatively sparser matrix.\\nFigure 3. Structure discovery in terms of SHD and FDR to the true\\ngraph, on synthetic data set generated by x=ATcos(x+1) +z.\\nFigure 4. Parameter estimates (before thresholding) of the graph\\non synthetic data set generated by x=ATcos(x+1) +z.\\nWe further experiment with a more complex nonlinear generation model, where the nonlinearity occurs after the linear\\ncombination of the variables, as opposed to the preceding\\ncase where nonlinearity is applied to the variables before\\nlinear combination. Speciﬁcally, we consider\\nx= 2 sin(AT(x+ 0:5\\x011)) +AT(x+ 0:5\\x011) +z;\\nand plot the results in Figure 5. One sees that with higher\\nnonlinearity, the proposed method results in signiﬁcantly\\nbetter SHD and FDR than does DAG-NOTEARS.\\nFigure 5. Structure discovery in terms of SHD and FDR to the true\\ngraph, on synthetic data set generated by x= 2 sin( AT(x+ 0:5\\x01\\n1)) +AT(x+ 0:5\\x011) +z.', 'The proposed method offers a modeling beneﬁt that the variables can be vector-valued with d>1. Moreover, since Z\\nresides in the latent space of the autoencoder and is not interpreted as noise as in linear SEM, one may take a smaller\\ncolumn dimension dZ<dif he/she believes that the variables have a lower intrinsic dimension. To demonstrate\\nthis capability, we construct a data set where the different\\ndimensions come from a randomly scaled and perturbedsample from linear SEM. Speciﬁcally, given a graph adjacency matrix A, we ﬁrst construct a sample ~x2Rm\\x021from\\nthe linear SEM ~x=AT~x+ ~z, and then generate for the\\nk-th dimension xk=uk~x+vk+zk, whereukandvkare\\nrandom scalars from standard normal and zkis a standard\\nnormal vector. The eventual sample is X= [x1jx2j\\x01\\x01\\x01jxd].\\nWe letd= 5 anddZ= 1 and compare DAG-GNN with\\nDAG-NOTEARS. The SHD and FDR are plotted in Figure 6.\\nThe ﬁgure clearly shows the signiﬁcantly better performance\\nof the proposed method. Moreover, the parameter estimates\\nare shown in Figure 7, compared against the ground-truth\\nA. One sees that the estimated graph from DAG-GNN\\nsuccessfully captures all the ground truth edges and that\\nthe estimated weights are also similar. On the other hand,\\nDAG-NOTEARS barely learns the graph.\\nFigure 6. Structure discovery in terms of SHD and FDR to the true\\ngraph, on synthetic vector-valued data set.\\nFigure 7. Parameter estimates (before thresholding) of the graph\\non synthetic vector-valued data set.', 'A beneﬁt of the proposed method is that it naturally handles discrete variables, a case precluded by linear SEM. We\\ndemonstrate the use of DAG-GNN on three discrete benchmark data sets: Child, Alarm, and Pigs (Tsamardinos et al.,\\n2006). For comparison is the state-of-the-art exact DAG\\nsolver GOPNILP (Cussens et al., 2016), which is based on a\\nconstrained integer programming formulation. We use 1000\\nsamples for learning.\\nOne sees from Table 1 that our results are reasonably close\\nto the ground truth, whereas not surprisingly the results\\nof GOPNILP are nearly optimal. The BIC score gap exhibits by DAG-GNN may be caused by the relatively simple\\nautoencoder architecture, which is less successful in approximating multinomial distributions. Nevertheless, it is\\nencouraging that the proposed method as a uniﬁed frame-\\nwork can handle discrete variables with only slight changes\\nin the network architecture.\\nTable 1. BIC scores on benchmark datasets of discrete variables.\\nDataset m Groundtruth GOPNILP DAG-GNN\\nChild 20 -1.27e+4 -1.27e+4 -1.38e+4\\nAlarm 37 -1.07e+4 -1.12e+4 -1.28e+4\\nPigs 441 -3.48e+5 -3.50e+5 -3.69e+5', 'We consider a bioinformatics data set (Sachs et al., 2005)\\nfor the discovery of a protein signaling network based on\\nexpression levels of proteins and phospholipids. This is\\na widely used data set for research on graphical models,\\nwith experimental annotations accepted by the biological\\nresearch community. The data set offers continuous measurements of expression levels of multiple phosphorylated\\nproteins and phospholipid components in human immune\\nsystem cells, and the modeled network provides the ordering\\nof the connections between pathway components. Based on\\nn= 7466 samples ofm= 11 cell types, Sachs et al. (2005)\\nestimate 20 edges in the graph.\\nIn Table 2, we compare DAG-GNN with DAG-NOTEARS\\nas well as FSG, the fast greedy search method proposed\\nby Ramsey et al. (2017), against the ground truth offered\\nby Sachs et al. (2005). The proposed method achieves the\\nlowest SHD. We further show in Figure 8 our estimated\\ngraph. One observes that it is acyclic. Our method successfully learns 8 out of 20 ground-truth edges (as marked by\\nred arrows), and predicts 5 indirectly connected edges (blue\\ndashed arrows) as well as 3 reverse edges (yellow arrows).\\nTable 2. Results on protein signaling network: comparison of the\\npredicted graphs with respect to the ground truth.\\nMethod SHD # Predicted edges\\nFGS 22 17\\nNOTEARS 22 16\\nDAG-GNN 19 18\\nFor another application, we develop a new causal inference task over relations deﬁned in a knowledge base (KB)\\nschema. The task aims at learning a BN, the nodes of which\\nare relations and the edges indicate whether one relation suggests another. For example, the relation person/Nationality\\nmay imply person/Language, because the spoken language\\nof a person naturally associates with his/her nationality. This\\ntask has a practical value, because most existing KBs are\\nconstructed by hand. The success of this task helps suggest\\nmeaningful relations for new entities and reduce human efforts. We construct a data set from FB15K-237 (Toutanova\\net al., 2015) and list in Table 3 a few extracted causal reFigure 8. Estimate protein signaling network.\\nlations. Because of space limitation, we defer the details\\nand more results in the supplementary material. One sees\\nthat these results are quite intuitive. We plan a comprehensive study with ﬁeld experts to systematically evaluate the\\nextraction results.\\nTable 3. Examples of extracted edges with high conﬁdence.\\nﬁlm/ProducedBy ) ﬁlm/Country\\nﬁlm/ProductionCompanies ) ﬁlm/Country\\nperson/Nationality ) person/Languages\\nperson/PlaceOfBirth ) person/Languages\\nperson/PlaceOfBirth ) person/Nationality\\nperson/PlaceLivedLocation ) person/Nationality', 'DAG structure learning is a challenging problem that has\\nlong been pursued in the literature of graphical models. The\\ndifﬁculty, in a large part, is owing to the NP-hardness incurred in the combinatorial formulation. Zheng et al. (2018)\\npropose an equivalent continuous constraint that opens the\\nopportunity of using well developed continuous optimization techniques for solving the problem. In this context, we\\nexplore the power of neural networks as functional approximators and develop a deep generative model to capture the\\ncomplex data distribution, aiming at better recovering the\\nunderlying DAG with a different design of the objective\\nfunction. In particular, we employ the machinery of variational autoencoders and parameterize them with new graph\\nneural network architectures. The proposed method handles\\nnot only data generated by parametric models beyond linear,\\nbut also variables in general forms, including scalar/vector\\nvalues and continuous/discrete types. We have performed\\nextensive experiments on synthetic, benchmark, and application data and demonstrated the practical competitiveness\\nof the proposal.', 'Bertsekas, D. P. Nonlinear Programming . Athena Scientiﬁc,\\n2nd edition, 1999.\\nBruna, J., Zaremba, W., Szlam, A., and LeCun, Y . Spectral\\nnetworks and locally connected networks on graphs. In\\nICLR , 2014.\\nChen, E. Y .-J., Shen, Y ., Choi, A., and Darwiche, A. Learning Bayesian networks with ancestral constraints. In\\nNIPS , 2016.\\nChen, J., Ma, T., and Xiao, C. FastGCN: Fast learning with\\ngraph convolutional networks via importance sampling.\\nInICLR , 2018.\\nChickering, D. M. Optimal structure identiﬁcation with\\ngreedy search. Journal of Machine Learning Research ,\\n2002.\\nChickering, D. M., Heckerman, D., and Meek, C. Largesample learning of Bayesian networks is NP-hard. Journal of Machine Learning Research , 5:1287–1330, 2004.\\nChow, C. and Liu, C. Approximating discrete probability\\ndistributions with dependence trees. IEEE transactions\\non Information Theory , 14(3):462–467, 1968.\\nCussens, J. Bayesian network learning with cutting planes.\\nInUAI, 2011.\\nCussens, J., Haws, D., and Studen `y, M. Polyhedral aspects of score equivalence in Bayesian network structure\\nlearning. Mathematical Programming , pp. 1–40, 2016.\\nDefferrard, M., Bresson, X., and Vandergheynst, P. Convolutional neural networks on graphs with fast localized\\nspectral ﬁltering. In NIPS , 2016.\\nEaton, D. and Murphy, K. Bayesian structure learning\\nusing dynamic programming and MCMC. arXiv preprint\\narXiv:1206.5247 , 2012.\\nFriedman, N. and Koller, D. Being Bayesian about network\\nstructure. a Bayesian approach to structure discovery in\\nBayesian networks. Machine learning , 50(1-2):95–125,\\n2003.\\nFriedman, N., Goldszmidt, M., and Wyner, A. Data analysis\\nwith Bayesian networks: A bootstrap approach. In UAI,\\n1999.\\nGilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and\\nDahl, G. E. Neural message passing for quantum chemistry. In ICML , 2017.\\nGrzegorczyk, M. and Husmeier, D. Improving the structure\\nMCMC sampler for Bayesian networks by introducing a\\nnew edge reversal move. Machine Learning , 71(2-3):265,\\n2008.Gmez, J., Mateo, J., and Puerta, J. Learning Bayesian\\nnetworks by hill climbing: efﬁcient methods based on\\nprogressive restriction of the neighborhood. Data Mining\\nand Knowledge Discovery , 22(1-2):106–148, 2011.\\nHamilton, W. L., Ying, R., and Leskovec, J. Inductive\\nrepresentation learning on large graphs. In NIPS , 2017.\\nHe, R., Tian, J., and Wu, H. Structure learning in Bayesian\\nnetworks of a moderate size by efﬁcient sampling. Journal of Machine Learning Research , 17(1):3483–3536,\\n2016.\\nHeckerman, D., Geiger, D., and Chickering, D. M. Learning\\nBayesian networks: The combination of knowledge and\\nstatistical data. Machine learning , 20(3):197–243, 1995.\\nJaakkola, T., Sontag, D., Globerson, A., and Meila, M.\\nLearning Bayesian network structure using LP relaxations. 2010.\\nKalainathan, D., Goudet, O., Guyon, I., Lopez-Paz, D., and\\nSebag, M. SAM: Structural agnostic model, causal discovery and penalized adversarial learning. arXiv preprint\\narXiv:1803.04929 , 2018.\\nKingma, D. P. and Ba, J. Adam: A method for stochastic\\noptimization. In ICLR , 2015.\\nKingma, D. P. and Welling, M. Auto-encoding variational\\nBayes. In ICLR , 2014.\\nKipf, T. N. and Welling, M. Semi-supervised classiﬁcation\\nwith graph convolutional networks. In ICLR , 2017.\\nKoivisto, M. and Sood, K. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning\\nResearch , 5:549–573, 2004.\\nLi, Y ., Tarlow, D., Brockschmidt, M., and Zemel, R. Gated\\ngraph sequence neural networks. In ICLR , 2016.\\nMadigan, D., York, J., and Allard, D. Bayesian graphical\\nmodels for discrete data. International Statistical Review/Revue Internationale de Statistique , pp. 215–232,\\n1995.\\nMohammadi, A., Wit, E. C., et al. Bayesian structure learning in sparse Gaussian graphical models. Bayesian Analysis, 10(1):109–138, 2015.\\nMohan, K., Chung, M., Han, S., Witten, D., Lee, S.-I.,\\nand Fazel, M. Structured learning of Gaussian graphical\\nmodels. In NIPS , 2012.\\nMooij, J. M., Magliacane, S., and Claassen, T. Joint\\ncausal inference from multiple contexts. arXiv preprint\\narXiv:1611.10351 , 2016.\\nNie, S., Mau ´a, D. D., De Campos, C. P., and Ji, Q. Advances\\nin learning Bayesian networks of bounded treewidth. In\\nNIPS , 2014.\\nNiinimaki, T., Parviainen, P., and Koivisto, M. Partial order\\nMCMC for structure discovery in Bayesian networks.\\narXiv preprint arXiv:1202.3753 , 2012.\\nNiinim ¨aki, T. M. and Koivisto, M. Annealed importance\\nsampling for structure learning in Bayesian networks. In\\nIJCAI , 2013.\\nOtt, S., Imoto, S., and Miyano, S. Finding optimal models for small gene networks. In Paciﬁc symposium on\\nbiocomputing , 2004.\\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E.,\\nDeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer,\\nA. Automatic differentiation in PyTorch. 2017.\\nPearl, J. Probabilistic reasoning in intelligent systems: networks of plausible inference . Morgan Kaufmann Publishers, Inc., 2 edition, 1988.\\nPearl, J. Causality: models, reasoning, and inference.\\nEconometric Theory , 19(46):675–685, 2003.\\nRamsey, J., Glymour, M., Sanchez-Romero, R., and Glymour, C. A million variables and more: the fast\\ngreedy equivalence search algorithm for learning highdimensional graphical causal models, with an application\\nto functional magnetic resonance images. International\\nJournal of Data Science and Analytics , 3(2):121–129,\\n2017.\\nSachs, K., Perez, O., Peer, D., Lauffenburger, D. A., and\\nNolan, G. P. Causal protein-signaling networks derived\\nfrom multiparameter single-cell data. Science , 308(5721):\\n523–529, 2005.\\nScanagatta, M., de Campos, C. P., Corani, G., and Zaffalon,\\nM. Learning Bayesian networks with thousands of variables. In NIPS , 2015.\\nSilander, T. and Myllymaki, P. A simple approach for ﬁnding the globally optimal Bayesian network structure. In\\nUAI, 2006.\\nSingh, A. P. and Moore, A. W. Finding optimal Bayesian\\nnetworks by dynamic programming. Technical report,\\nCarnegie Mellon University, 2005.\\nSpirtes, P., Meek, C., and Richardson, T. Causal inference\\nin the presence of latent variables and selection bias. In\\nUAI, 1995.\\nSpirtes, P., Glymour, C. N., and Scheines, R. Computation,\\nCausation, and Discovery . AAAI Press, 1999.Spirtes, P., Glymour, C., Scheines, R., Kauffman, S.,\\nAimale, V ., and Wimberly, F. Constructing Bayesian\\nnetwork models of gene expression networks from microarray data, 2000a.\\nSpirtes, P., Glymour, C. N., Scheines, R., Heckerman, D.,\\nMeek, C., Cooper, G., and Richardson, T. Causation,\\nprediction, and search . MIT press, 2000b.\\nToutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury,\\nP., and Gamon, M. Representing text for joint embedding\\nof text and knowledge bases. In EMNLP , 2015.\\nTsamardinos, I., Aliferis, C. F., and Statnikov, A. Time and\\nsample efﬁcient discovery of markov blankets and direct\\ncausal relations. In SIGKDD , 2003.\\nTsamardinos, I., Brown, L., and Aliferis, C. The maxmin hill-climbing Bayesian network structure learning\\nalgorithm. Machine Learning , 65(1):31–78, 2006.\\nVeli˘ckovi ´c, P., Cucurull, G., Casanova, A., Romero, A., Li `o,\\nP., and Bengio, Y . Graph attention networks. In ICLR ,\\n2018.\\nYuan, C. and Malone, B. Learning optimal Bayesian networks: A shortest path perspective. 48:23–65, 2013.\\nZhang, J. On the completeness of orientation rules for\\ncausal discovery in the presence of latent confounders\\nand selection bias. Artiﬁcial Intelligence , 172(16-17):\\n1873–1896, 2008.\\nZheng, X., Aragam, B., Ravikumar, P., and Xing, E. P.\\nDAGs with NO TEARS: Continuous optimization for\\nstructure learning. In NIPS , 2018.\\nA. Proofs\\nProof of Theorem 1. LetB=A\\x0eA. Clearly,Bis nonnegative. The binomial expansion reads\\n(I+\\x0bB)m=I+mX\\nk=1\\x12m\\nk\\x13\\n\\x0bkBk:\\nIt is known that there is a cycle of length kif and only if\\ntr(Bk)>0whenB\\x150. Because if there is a cycle then\\nthere is a cycle of length at most m, we conclude that there\\nis no cycle if and only if tr[(I+\\x0bB)m] = tr(I) =m.\\nProof of Theorem 2. Write\\n(1 +\\x0bj\\x15j)m=\\x12\\n1 +cj\\x15j\\nm\\x13m\\n:\\nFor givencandj\\x15j, the right-hand side of the equality is\\na function of m. This function monotonically increases\\nfor positive mand has a limit ecj\\x15j. Hence, for any ﬁnite\\nm> 0,(1 +\\x0bj\\x15j)m\\x14ecj\\x15j.\\nB. Structure Learning over KB Relations\\nWe construct the data set from triples in FB15K-237\\n(Toutanova et al., 2015), which is a subset of FreeBase with\\napproximately 15k entities and 237 relations. Each sample\\ncorresponds to an entity and each variable corresponds to a\\nrelation in this knowledge base. Each sample has on average', '\\n7.36 relations (i.e. 7.36 non-zero entries in each row).\\n', 'Table 4 gives additional examples learned by our model with\\nhighest conﬁdence scores. For each target relation on the\\nright-hand side, we show the highest ranked relations within\\nthe same domain (i.e. the contents in the ﬁeld before “/”\\nsuch as “ﬁlm” and “tvProgram”). On the left-hand side, we\\nomit the relations that are common to the associated entity\\ntypes, e.g. “profession” and “gender” to persons and “genre”\\nto ﬁlms, because almost all entities with these types will\\ncontain such a relation.\\nTable 4. (Continued from Table 3) Examples of extracted edges with high conﬁdence. The dot \\x01appearing in R1:R2means that the sample\\nentity is connected to a virtual node (i.e. compound value types in FreeBase) via relation R1, followed by a relation R2to a real entity.\\nﬁlm/ProducedBy ) ﬁlm/Country\\nﬁlm/ProductionCompanies ) ﬁlm/Country\\ntvProgram/CountryOfOriginal ) tvProgram/Language\\ntvProgram/RegularCast.regularTv/AppearanceActor ) tvProgram/Language\\nperson/Nationality ) person/Languages\\nperson/PlaceOfBirth ) person/Languages\\nperson/PlaceOfBirth ) person/Nationality\\nperson/PlaceLivedLocation ) person/Nationality\\norganization/Headquarters.mailingAddress/Citytown ) organization/PlaceFounded\\norganization/Headquarters.mailingAddress/StateProvince ) organization/PlaceFounded']\n"
     ]
    }
   ],
   "source": [
    "docs = text_to_docs(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Learning a faithful directed acyclic graph (DAG)\\nfrom samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the\\nnumber of graph nodes. A recent breakthrough\\nformulates the problem as a continuous optimization with a structural constraint that ensures\\nacyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation\\nmodel (SEM) and the least-squares loss function\\nthat are statistically well justiﬁed but nevertheless\\nlimited. Motivated by the widespread success of\\ndeep learning that is capable of capturing complex nonlinear mappings, in this work we propose\\na deep generative model and apply a variant of\\nthe structural constraint to learn the DAG. At the\\nheart of the generative model is a variational autoencoder parameterized by a novel graph neural\\nnetwork architecture, which we coin DAG-GNN.\\nIn addition to the richer capacity, an advantage\\nof the proposed model is that it naturally handles discrete variables as well as vector-valued\\nones. We demonstrate that on synthetic data\\nsets, the proposed method learns more accurate\\ngraphs for nonlinearly generated samples; and on\\nbenchmark data sets with discrete variables, the\\nlearned graphs are reasonably close to the global\\noptima. The code is available at https://\\ngithub.com/fishmoon1234/DAG-GNN .', metadata={'page': 1, 'chunk': 0, 'source': '1-0'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = parse_pdf(open('file.pdf', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiters = '(\\nAbstract\\n)|(\\n.*Introduction\\n)|(\\n.*Background and Related Work\\n)|(\\n.*Conclusion\\n)|(\\n.*Method\\n)|(\\n.*Related Work\\n)|(\\n.*Evaluation\\n)|(\\n.*Experiments\\n)|(\\n.*Discussion\\n)|(\\n.*Limitations\\n)|(\\n.*References\\n)|(\\n[0-9]\\..*\\n)'\n",
    "text_split = re.split(delimiters, full_text)\n",
    "text = [item for item in text_split if item is not None]\n",
    "text = [item for item in text if len(item)>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_docs = [Document(page_content=page) for page in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(page_docs):\n",
    "    doc.metadata[\"page\"] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(page_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for doc in page_docs:\n",
    "    t.append(num_tokens_from_string(doc.page_content, 'text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11612"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chunks = []\n",
    "for doc in page_docs:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "        chunk_overlap=0,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk, metadata={\"page\": doc.metadata[\"page\"], \"chunk\": i}\n",
    "        )\n",
    "        # Add sources a metadata\n",
    "        doc.metadata[\"source\"] = f\"{doc.metadata['page']}-{doc.metadata['chunk']}\"\n",
    "        doc_chunks.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for doc in doc_chunks:\n",
    "    t.append(num_tokens_from_string(doc.page_content, 'text-embedding-ada-002'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11580"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Yue Yu* 1Jie Chen* 2 3Tian Gao3Mo Yu3 Abstract Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justiﬁed but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at https:// github.com/fishmoon1234/DAG-GNN . 1. Introduction Bayesian Networks (BN) have been widely used in machine learning applications (Spirtes et al., 1999; Ott et al., 2004). The structure of a BN takes the form of a directed acyclic graph (DAG) and plays a vital part in causal inference (Pearl, *Equal contribution1Lehigh University2MIT-IBM Watson AI Lab3IBM Research. Correspondence to: Yue Yu <yuy214@lehigh.edu >, Jie Chen <chenjie@us.ibm.com >. Proceedings of the 36thInternational Conference on Machine Learning , Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).1988) with many applications in medicine, genetics, economics, and epidemics. Its structure learning problem is however NP-hard (Chickering et al., 2004) and stimulates a proliferation of literature. Score-based methods generally formulate the structure learning problem as optimizing a certain score function with respect to the unknown (weighted) adjacency matrix Aand the observed data samples, with a combinatorial constraint stating that the graph must be acyclic. The intractable search space (with a complexity superexponential in the number of graph nodes) poses substantial challenges for optimization. Hence, for practical problems in a scale beyond small, approximate search often needs to be employed with additional structure assumption (Nie et al., 2014; Chow & Liu, 1968; Scanagatta et al., 2015; Chen et al., 2016). Recently, Zheng et al. (2018) formulate an equivalent acyclicity constraint by using a continuous function of the adjacency matrix (speciﬁcally, the matrix exponential of A\\x0eA). This approach drastically changes the combinatorial nature of the problem to a continuous optimization, which may be efﬁciently solved by using maturely developed blackbox solvers. The optimization problem is nevertheless nonlinear, thus these solvers generally return only a stationary-point solution rather than the global optimum. Nevertheless, the authors show that empirically such local solutions are highly comparable to the global ones obtained through expensive combinatorial search. With the inspiring reformulation of the constraint, we revisit the objective function. The score-based objective functions generally make assumptions of the variables and the model class. For example, Zheng et al. (2018) demonstrate on the linear structural equation model (SEM) with a leastsquares loss. While convenient, such assumptions are often restricted and they may not correctly reﬂect the actual distribution of real-life data. Hence, motivated by the remarkable success of deep neural networks, which are arguably universal approximators, in this work we develop a graph-based deep generative model aiming at better capturing the sampling distribution faithful to the DAG. To this end, we employ the machinery of variational inference and parameterize a pair of encoder/decoder with specially designed graph neural networks (GNN). The objective function (the score), then, is the evidence lowerarXiv:1904.10098v1  [cs.LG]  22 Apr 2019',\n",
       " ' bound. Different from the current ﬂourishing designs of GNNs (Bruna et al., 2014; Defferrard et al., 2016; Li et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Chen et al., 2018; Veli ˘ckovi ´c et al., 2018), the proposed ones are generalized from linear SEM, so that the new model performs at least as well as linear SEM when the data is linear. Our proposal has the following distinct features and advantages. First, the work is built on the widespread use of deep generative models (speciﬁcally, variational autoencoders, V AE (Kingma & Welling, 2014)) that are able to capture complex distributions of data and to sample from them. Under the graph setting, the weighted adjacency matrix is an explicit parameter, rather than a latent structure, learnable together with other neural network parameters. The proposed network architecture has not been used before. Second, the framework of V AE naturally handles various data types, notably not only continuous but also discrete ones. All one needs to do is to model the likelihood distribution (decoder output) consistent with the nature of the variables. Third, owing to the use of graph neural networks for parameterization, each variable (node) can be not only scalar-valued but also vector-valued. These variables are considered node features input to/output of the GNNs. Fourth, we propose a variant of the acyclicity constraint more suitable for implementation under current deep learning platforms. The matrix exponential suggested by Zheng et al. (2018), while mathematically elegant, may not be implemented or supported with automatic differentiation in all popular platforms. We propose a polynomial alternative more practically convenient and as numerically stable as the exponential. We demonstrate the effectiveness of the proposed method on synthetic data generated from linear and nonlinear SEMs, benchmark data sets with discrete variables, and data sets from applications. For synthetic data, the proposed DAGGNN outperforms DAG-NOTEARS, the algorithm proposed by Zheng et al. (2018) based on linear SEM. For benchmark data, our learned graphs compare favorably with those obtained through optimizing the Bayesian information criterion by using combinatorial search. 2. Background and Related Work A DAGGand a joint distribution Parefaithful to each other if all and only the conditional independencies true in Pare entailed by G(Pearl, 1988). The faithfulness condition enables one to recover GfromP. Given independent and iid samplesDfrom an unknown distribution corresponding to a faithful but unknown DAG, structure learning refers torecovering the DAG from D. Many exact and approximate algorithms for learning DAG from data have been developed, including score-based and constraint-based approaches (Spirtes et al., 2000a; Chickering, 2002; Koivisto & Sood, 2004; Silander & Myllymaki, 2006; Jaakkola et al., 2010; Cussens, 2011; Yuan & Malone, 2013). Score-based methods generally use a score to measure the goodness of ﬁt of different graphs over data; and then use a search procedure—such as hill-climbing (Heckerman et al., 1995; Tsamardinos et al., 2006; Gmez et al., 2011), forward-backward search (Chickering, 2002), dynamic programming (Singh & Moore, 2005; Silander & Myllymaki, 2006), A\\x03(Yuan & Malone, 2013), or integer programming (Jaakkola et al., 2010; Cussens, 2011; Cussens et al., 2016)—in order to ﬁnd the best graph. Commonly used Bayesian score criteria, such as BDeu and Bayesian information criterion (BIC), are decomposable, consistent, locally consistent (Chickering, 2002), and score equivalent (Heckerman et al., 1995). To make the DAG search space tractable, approximate methods make additional assumptions such as bounded tree-width (Nie et al., 2014), tree-like structures (Chow & Liu, 1968), approximation (Scanagatta et al., 2015), and other constraints about the DAG (Chen et al., 2016). Many bootstrap (Friedman et al., 1999) and sampling-based structure learning algorithms (Madigan et al., 1995; Friedman & Koller, 2003; Eaton & Murphy, 2012; Grzegorczyk & Husmeier, 2008; Niinim ¨aki & Koivisto, 2013; Niinimaki et al., 2012; He et al., 2016) are also proposed to tackle the expensive search problem. Constraint-based methods, in contrast, use (conditional) independence tests to test the existence of edges between each pair of variables. Popular algorithms include SGS (Spirtes et al., 2000b), PC (Spirtes et al., 2000b), IC (Pearl, 2003), and FCI (Spirtes et al., 1995; Zhang, 2008). Recently, there appears a suite of hybrid algorithms that combine score-based and constraint-based methods, such as MMHC (Tsamardinos et al., 2003), and apply constraintbased methods to multiple environments (Mooij et al., 2016). Due to the NP-hardness, traditional DAG learning methods usually deal with discrete variables, as discussed above, or jointly Gaussian variables (Mohan et al., 2012; Mohammadi et al., 2015). Recently, a new continuous optimization approach is proposed (Zheng et al., 2018), which transforms the discrete search procedure into an equality constraint. This approach enables a suite of continuous optimization techniques such as gradient descent to be used. The approach achieves good structure recovery results, although it is applied to only linear SEM for ease of exposition. Neural-network approaches started to surface only very re-',\n",
       " ' cently. Kalainathan et al. (2018) propose a GAN-style (generative adversarial network) method, whereby a separate generative model is applied to each variable and a discriminator is used to distinguish between the joint distributions of real and generated samples. The approach appears to scale well but acyclicity is not enforced. 3. Neural DAG Structure Learning Our method learns the weighted adjacency matrix of a DAG by using a deep generative model that generalizes linear SEM, with which we start the journey. 3.1. Linear Structural Equation Model LetA2Rm\\x02mbe the weighted adjacency matrix of the DAG withmnodes andX2Rm\\x02dbe a sample of a joint distribution of mvariables, where each row corresponds to one variable. In the literature, a variable is typically a scalar, but it can be trivially generalized to a d-dimensional vector under the current setting. The linear SEM model reads X=ATX+Z; (1) whereZ2Rm\\x02dis the noise matrix. When the graph nodes are sorted in the topological order, the matrix Ais strictly upper triangular. Hence, ancestral sampling from the DAG is equivalent to generating a random noise Zfollowed by a triangular solve X= (I\\x00AT)\\x001Z: (2) 3.2. Proposed Graph Neural Network Model Equation (2)may be written as X=fA(Z), a general form recognized by the deep learning community as an abstraction of parameterized graph neural networks that take node features Zas input and return Xas high level representations. Nearly all graph neural networks (Bruna et al., 2014; Defferrard et al., 2016; Li et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Gilmer et al., 2017; Chen et al., 2018; Veli ˘ckovi ´c et al., 2018) can be written in this form. For example, the popular GCN (Kipf & Welling, 2017) architecture reads X=bA\\x01ReLU(bAZW1)\\x01W2; wherebAis a normalization of AandW1andW2are parameter matrices. Owing to the special structure (2), we propose a new graph neural network architecture X=f2((I\\x00AT)\\x001f1(Z)): (3) The parameterized functions f1andf2effectively perform (possibly nonlinear) transforms on ZandX, respectively. If f2is invertible, then (3)is equivalent tof\\x001 2(X) =ATf\\x001 2(X) +f1(Z), a generalized version of the linear SEM (1). We will defer the instantiation of these functions in a later subsection. One of the reasons is that the activation in f2must match the type of the variable X, a subject to be discussed together with discrete variables. 3.3. Model Learning with Variational Autoencoder Given a speciﬁcation of the distribution of Zand samplesX1;:::;Xn, one may learn the generative model (3) through maximizing the log-evidence 1 nnX k=1logp(Xk) =1 nnX k=1logZ p(XkjZ)p(Z)dZ; which, unfortunately, is generally intractable. Hence, we appeal to variational Bayes. To this end, we use a variational posterior q(ZjX)to approximate the actual posterior p(ZjX). The net result is the evidence lower bound (ELBO) LELBO =1 nnX k=1Lk ELBO; with Lk ELBO\\x11\\x00DKL\\x10 q(ZjXk)jjp(Z)\\x11 + Eq(ZjXk)h logp(XkjZ)i :(4) Each individual term Lk ELBO departs from the log-evidence byDKL\\x10 q(ZjXk)jjp(ZjXk)\\x11 \\x150, the KL-divergence between the variational posterior and the actual one. The ELBO lends itself to a variational autoencoder (V AE) (Kingma & Welling, 2014), where given a sample Xk, the encoder (inference model) encodes it into a latent variableZwith density q(ZjXk); and the decoder (generative model) tries to reconstruct XkfromZwith density p(XkjZ). Both densities may be parameterized by using neural networks. Modulo the probability speciﬁcation to be completed later, the generative model (3)discussed in the preceding subsection plays the role of the decoder. Then, we propose the corresponding encoder Z=f4((I\\x00AT)f3(X)); (5) wheref3andf4are parameterized functions that conceptually play the inverse role of f2andf1, respectively. 3.4. Architecture and Loss Function To complete the V AE, one must specify the distributions in(4). Recall that for now both XkandZarem\\x02dmatrices.',\n",
       " ' ENCODERDECODERMLP MLP Figure 1. Architecture (for continuous variables). In the case of discrete variables, the decoder output is changed from MX; SXtoPX. For simplicity, the prior is typically modeled as the standard matrix normal p(Z) =MNm\\x02d(0;I;I). For the inference model, we let f3be a multilayer perceptron (MLP) andf4be the identity mapping. Then, the variational posteriorq(ZjX)is a factored Gaussian with mean MZ2 Rm\\x02dand standard deviation SZ2Rm\\x02d, computed from the encoder [MZjlogSZ] = (I\\x00AT) MLP(X;W1;W2);(6) where MLP(X;W1;W2) := ReLU( XW1)W2, andW1 andW2are parameter matrices. For the generative model, we let f1be the identity mapping andf2be an MLP. Then, the likelihood p(XjZ)is a factored Gaussian with mean MX2Rm\\x02dand standard deviation SX2Rm\\x02d, computed from the decoder [MXjlogSX] = MLP((I\\x00AT)\\x001Z;W3;W4);(7) whereW3andW4are parameter matrices. One may switch the MLP and the identity mapping inside each of the encoder/decoder, but we ﬁnd that the performance is less competitive. One possible reason is that the current design (7)places an emphasis on the nonlinear transform of a sample (I\\x00AT)\\x001Zfrom linear SEM, which better captures nonlinearity. Based on (6)and (7), the KL-divergence term in the ELBO (4) admits a closed form DKL\\x10 q(ZjX)jjp(Z)\\x11 = 1 2mX i=1dX j=1(SZ)2 ij+ (MZ)2 ij\\x002 log(SZ)ij\\x001;(8) and the reconstruction accuracy term may be computed with Monte Carlo approximation Eq(ZjX)h logp(XjZ)i \\x19 1 LLX l=1mX i=1dX j=1\\x00(Xij\\x00(M(l) X)ij)2 2(S(l) X)2 ij\\x00log(S(l) X)ij\\x00c; (9)wherecis a constant and M(l) XandS(l) Xare the outputs of the decoder (7)by taking as input Monte Carlo samples Z(l)\\x18q(ZjX), forl= 1;:::;L . Note that under the autoencoder framework, Zis considered latent (rather than the noise in linear SEM). Hence, the column dimension of Zmay be different from d. From the neural network point of view, changing the column dimension ofZaffects only the sizes of the parameter matrices W2andW3. Sometimes, one may want to use a smaller number than dif he/she observes that the data has a smaller intrinsic dimension. An illustration of the architecture is shown in Figure 1. 3.5. Discrete Variables One advantage of the proposed method is that it naturally handles discrete variables. We assume that each variable has a ﬁnite support of cardinality d. Hence, we let each row of Xbe a one-hot vector, where the “on” location indicates the value of the corresponding variable. We still use standard matrix normal to model the prior and factored Gaussian to model the variational posterior, with (6)being the encoder. On the other hand, we need to slightly modify the likelihood to cope with the discrete nature of the variables. Speciﬁcally, we let p(XjZ)be a factored categorical distribution with probability matrix PX, where each row is a probability vector for the corresponding categorical variable. To achieve so, we change f2from the identity mapping to a row-wise softmax and modify the decoder (7) to PX= softmax(MLP(( I\\x00AT)\\x001Z;W3;W4)):(10) Correspondingly for the ELBO, the KL term (8)remains the same, but the reconstruction term (9)needs be modiﬁed to Eq(ZjX)h logp(XjZ)i \\x191 LLX l=1mX i=1dX j=1Xijlog(P(l) X)ij; (11) whereP(l) Xis the output of the decoder (10) by taking as input Monte Carlo samples Z(l)\\x18q(ZjX), forl= 1;:::;L .',\n",
       " ' 3.6. Connection to Linear SEM One has seen from the forgoing discussions how the proposed model is developed from linear SEM: We apply nonlinearality to the sampling procedure (2)of SEM, treat the resulting generative model as a decoder, and pair with it a variational encoder for tractable learning. Compared with a plain autoencoder, the variational version allows a modeling of the latent space, from which samples are generated. We now proceed, in a reverse thought ﬂow, to establish the connection between the loss function of the linear SEM considered in Zheng et al. (2018) and that of ours. We ﬁrst strip off the variational component of the autoencoder. This plain version uses (5)as the encoder and (3)as the decoder. For notational clarity, let us write bXas the output of the decoder, to distinguish it from the encoder input X. A typical sample loss to minimize is 1 2mX i=1dX j=1(Xij\\x00bXij)2+1 2mX i=1dX j=1Z2 ij; where the ﬁrst term is the reconstruction error and the second term is a regularization of the latent space. One recognizes that the reconstruction error is the same as the negative reconstruction accuracy (9)in the ELBO, up to a constant, if the standard deviation SXis1, the meanMXis taken asbX, and only one Monte Carlo sample is drawn from the variational posterior. Moreover, the regularization term is the same as the KL-divergence (8)in the ELBO if the standard deviation SZis1and the mean MZis taken asZ. If we further strip off the (possibly nonlinear) mappings f1 tof4, then the encoder (5)and decoder (3)read, respectively, Z= (I\\x00AT)XandbX= (I\\x00AT)\\x001Z. This pair results in perfect reconstruction, and hence the sample loss reduces to 1 2mX i=1dX j=1Z2 ij=1 2k(I\\x00AT)Xk2 F; (12) which is the least-squares loss used and justiﬁed by Zheng et al. (2018). 3.7. Acyclicity Constraint Neither maximizing the ELBO (4)nor minimizing the leastsquares loss (12) guarantees that the corresponding graph of the resulting Ais acyclic. Zheng et al. (2018) pair the loss function with an equality constraint, whose satisfaction ensures acyclicity. The idea is based on the fact that the positivity of the (i;j) element of the k-th power of a nonnegative adjacency matrixBindicates the existence of a length- kpath between nodesiandj. Hence, the positivity of the diagonal of Bk reveals cycles. The authors leverage the trick that the matrixexponential admits a Taylor series (because it is analytic on the complex plane), which is nothing but a weighted sum of all nonnegative integer powers of the matrix. The coefﬁcient of the zeorth power (the identity matrix Im\\x02m) is1, and hence the trace of the exponential of Bmust be exactly m for a DAG. To satisfy nonnegativity, one may let Bbe the elementwise square of A; that is,B=A\\x0eA. Whereas the formulation of this acyclicity constraint is mathematically elegant, support of the matrix exponential may not be available in all deep learning platforms. To ease the coding effort, we propose an alternative constraint that is practically convenient. Theorem 1. LetA2Rm\\x02mbe the (possibly negatively) weighted adjacency matrix of a directed graph. For any \\x0b>0, the graph is acyclic if and only if tr[(I+\\x0bA\\x0eA)m]\\x00m= 0: (13) We use (13) as the equality constraint when maximizing the ELBO. The computations of both (I+\\x0bB)mandexp(B) may meet numerical difﬁculty when the eigenvalues of B have a large magnitude. However, the former is less severe than the latter with a judicious choice of \\x0b. Theorem 2. Let\\x0b=c=m> 0for somec. Then for any complex\\x15, we have (1 +\\x0bj\\x15j)m\\x14ecj\\x15j. In practice, \\x0bmay be treated as a hyperparameter and its setting depends on an estimation of the largest eigenvalue of Bin magnitude. This value is the spectral radius of B, and because of nonnegativity, it is bounded by the maximum row sum according to the Perron–Frobenius theorem. 3.8. Training Based on the foregoing, the learning problem is min A;\\x12f(A;\\x12)\\x11\\x00LELBO s.t.h(A)\\x11tr[(I+\\x0bA\\x0eA)m]\\x00m= 0; where the unknowns include the matrix Aand all the parameters \\x12of the V AE (currently we have \\x12= fW1;W2;W3;W4g). Nonlinear equality-constrained problems are well studied and we use the augmented Lagrangian approach to solve it. For completeness, we summarize the algorithm here; the reader is referred to standard textbooks such as Section 4.2 of Bertsekas (1999) for details and convergence analysis. Deﬁne the augmented Lagrangian Lc(A;\\x12;\\x15 ) =f(A;\\x12) +\\x15h(A) +c 2jh(A)j2; where\\x15is the Lagrange multiplier and cis the penalty parameter. When c= +1, the minimizer of Lc(A;\\x12;\\x15 )',\n",
       " ' must satisfy h(A) = 0 , in which case Lc(A;\\x12;\\x15 )is equal to the objective function f(A;\\x12). Hence, the strategy is to progressively increase c, for each of which minimize the unconstrained augmented Lagrangian. The Lagrange multiplier\\x15is correspondingly updated so that it converges to the one under the optimality condition. There exist a few variants for updating \\x15and increasing c, but a typical effective rule reads: (Ak;\\x12k) = argmin A;\\x12Lck(A;\\x12;\\x15k); (14) \\x15k+1=\\x15k+ckh(Ak); (15) ck+1=( \\x11ck;ifjh(Ak)j>\\rjh(Ak\\x001)j; ck;otherwise;(16) where\\x11>1and\\r <1are tuning parameters. We ﬁnd that often\\x11= 10 and\\r= 1=4work well. The subproblem (14) may be solved by using blackbox stochastic optimization solvers, by noting that the ELBO is deﬁned on a set of samples. 4. Experiments In this section, we present a comprehensive set of experiments to demonstrate the effectiveness of the proposed method DAG-GNN. In Section 4.1, we compare with DAGNOTEARS, the method proposed by Zheng et al. (2018) based on linear SEM, on synthetic data sets generated by sampling generalized linear models, with an emphasis on nonlinear data and vector-valued data ( d > 1). In Section 4.2, we showcase the capability of our model with discrete data, often seen in benchmark data sets with ground truths for assessing quality. To further illustrate the usefulness of the proposed method, in Section 4.3 we apply DAG-GNN on a protein data set for the discovery of consensus protein signaling network, as well as a knowledge base data set for learning causal relations. Our implementation is based on PyTorch (Paszke et al., 2017). We use Adam (Kingma & Ba, 2015) to solve the subproblems (14). To avoid overparameterization, we parameterize the variational posterior q(ZjX)as a factored Gaussian with constant unit variance, and similarly for the likelihoodp(XjZ). When extracting the DAG, we use a thresholding value 0:3, following the recommendation of Zheng et al. (2018). For benchmark and application data sets, we include a Huber-norm regularization of Ain the objective function to encourage more rapid convergence. 4.1. Synthetic Data Sets The synthetic data sets are generated in the following manner. We ﬁrst generate a random DAG by using the Erd ˝os– R´enyi model with expected node degree 3, then assign uni-formly random weights for the edges to obtain the weighted adjacency matrix A. A sampleXis generated by sampling the (generalized) linear model X=g(ATX)+Zwith some functiongelaborated soon. The noise Zfollows standard matrix normal. When the dimension d= 1, we use lowercase letters to denote vectors; that is, x=g(ATx) +z. We compare DAG-GNN with DAG-NOTEARS and report the structural Hamming distance (SHD) and false discovery rate (FDR), each averaged over ﬁve random repetitions. With sample size n= 5000 , we run experiments on four graph sizesm2f10;20;50;100g. In Sections 4.1.1 and 4.1.2 we consider scalar-valued variables ( d= 1) and in Section 4.1.3 vector-valued variables ( d>1). 4.1.1. L INEAR CASE This case is the linear SEM model, with gbeing the identity mapping. The SHD and FDR are plotted in Figure 2. One sees that the graphs learned by the proposed method are substantially more accurate than those by DAG-NOTEARS when the graphs are large. Figure 2. Structure discovery in terms of SHD and FDR to the true graph, on synthetic data set generated by x=ATx+z. 4.1.2. N ONLINEAR CASE We now consider data generated by the following model x=ATh(x) +z; for some nonlinear function h. Taking ﬁrst-order approximationh(x)\\x19h(0)1+h0(0)x(ignoring higher-order terms of x), one obtains an amendatory approximation of the graph adjacency matrix, h0(0)A. This approximate ground truth maintains the DAG structure, with only a scaling on the edge weights. We takeh(x) = cos(x+1)and plot the SHD and FDR in Figure 3. one observes that DAG-GNN slightly improves over DAG-NOTEARS in terms of SHD. Further, FDR is substantially improved, by approximately a factor of three, which indicates that DAG-GNN tends to be more accurate on selecting correct edges. This observation is consistent with the parameter estimates shown in Figure 4, where the ground truth is set as \\x00sin(1)A. The heat map conﬁrms that DAG-GNN results in fewer “false alarms” and recovers a relatively sparser matrix.',\n",
       " ' Figure 3. Structure discovery in terms of SHD and FDR to the true graph, on synthetic data set generated by x=ATcos(x+1) +z. Figure 4. Parameter estimates (before thresholding) of the graph on synthetic data set generated by x=ATcos(x+1) +z. We further experiment with a more complex nonlinear generation model, where the nonlinearity occurs after the linear combination of the variables, as opposed to the preceding case where nonlinearity is applied to the variables before linear combination. Speciﬁcally, we consider x= 2 sin(AT(x+ 0:5\\x011)) +AT(x+ 0:5\\x011) +z; and plot the results in Figure 5. One sees that with higher nonlinearity, the proposed method results in signiﬁcantly better SHD and FDR than does DAG-NOTEARS. Figure 5. Structure discovery in terms of SHD and FDR to the true graph, on synthetic data set generated by x= 2 sin( AT(x+ 0:5\\x01 1)) +AT(x+ 0:5\\x011) +z. 4.1.3. V ECTOR -VALUED CASE The proposed method offers a modeling beneﬁt that the variables can be vector-valued with d>1. Moreover, since Z resides in the latent space of the autoencoder and is not interpreted as noise as in linear SEM, one may take a smaller column dimension dZ<dif he/she believes that the variables have a lower intrinsic dimension. To demonstrate this capability, we construct a data set where the different dimensions come from a randomly scaled and perturbedsample from linear SEM. Speciﬁcally, given a graph adjacency matrix A, we ﬁrst construct a sample ~x2Rm\\x021from the linear SEM ~x=AT~x+ ~z, and then generate for the k-th dimension xk=uk~x+vk+zk, whereukandvkare random scalars from standard normal and zkis a standard normal vector. The eventual sample is X= [x1jx2j\\x01\\x01\\x01jxd]. We letd= 5 anddZ= 1 and compare DAG-GNN with DAG-NOTEARS. The SHD and FDR are plotted in Figure 6. The ﬁgure clearly shows the signiﬁcantly better performance of the proposed method. Moreover, the parameter estimates are shown in Figure 7, compared against the ground-truth A. One sees that the estimated graph from DAG-GNN successfully captures all the ground truth edges and that the estimated weights are also similar. On the other hand, DAG-NOTEARS barely learns the graph. Figure 6. Structure discovery in terms of SHD and FDR to the true graph, on synthetic vector-valued data set. Figure 7. Parameter estimates (before thresholding) of the graph on synthetic vector-valued data set. 4.2. Benchmark Data Sets A beneﬁt of the proposed method is that it naturally handles discrete variables, a case precluded by linear SEM. We demonstrate the use of DAG-GNN on three discrete benchmark data sets: Child, Alarm, and Pigs (Tsamardinos et al., 2006). For comparison is the state-of-the-art exact DAG solver GOPNILP (Cussens et al., 2016), which is based on a constrained integer programming formulation. We use 1000 samples for learning. One sees from Table 1 that our results are reasonably close to the ground truth, whereas not surprisingly the results of GOPNILP are nearly optimal. The BIC score gap exhibits by DAG-GNN may be caused by the relatively simple autoencoder architecture, which is less successful in approximating multinomial distributions. Nevertheless, it is encouraging that the proposed method as a uniﬁed frame-',\n",
       " ' work can handle discrete variables with only slight changes in the network architecture. Table 1. BIC scores on benchmark datasets of discrete variables. Dataset m Groundtruth GOPNILP DAG-GNN Child 20 -1.27e+4 -1.27e+4 -1.38e+4 Alarm 37 -1.07e+4 -1.12e+4 -1.28e+4 Pigs 441 -3.48e+5 -3.50e+5 -3.69e+5 4.3. Applications We consider a bioinformatics data set (Sachs et al., 2005) for the discovery of a protein signaling network based on expression levels of proteins and phospholipids. This is a widely used data set for research on graphical models, with experimental annotations accepted by the biological research community. The data set offers continuous measurements of expression levels of multiple phosphorylated proteins and phospholipid components in human immune system cells, and the modeled network provides the ordering of the connections between pathway components. Based on n= 7466 samples ofm= 11 cell types, Sachs et al. (2005) estimate 20 edges in the graph. In Table 2, we compare DAG-GNN with DAG-NOTEARS as well as FSG, the fast greedy search method proposed by Ramsey et al. (2017), against the ground truth offered by Sachs et al. (2005). The proposed method achieves the lowest SHD. We further show in Figure 8 our estimated graph. One observes that it is acyclic. Our method successfully learns 8 out of 20 ground-truth edges (as marked by red arrows), and predicts 5 indirectly connected edges (blue dashed arrows) as well as 3 reverse edges (yellow arrows). Table 2. Results on protein signaling network: comparison of the predicted graphs with respect to the ground truth. Method SHD # Predicted edges FGS 22 17 NOTEARS 22 16 DAG-GNN 19 18 For another application, we develop a new causal inference task over relations deﬁned in a knowledge base (KB) schema. The task aims at learning a BN, the nodes of which are relations and the edges indicate whether one relation suggests another. For example, the relation person/Nationality may imply person/Language, because the spoken language of a person naturally associates with his/her nationality. This task has a practical value, because most existing KBs are constructed by hand. The success of this task helps suggest meaningful relations for new entities and reduce human efforts. We construct a data set from FB15K-237 (Toutanova et al., 2015) and list in Table 3 a few extracted causal reFigure 8. Estimate protein signaling network. lations. Because of space limitation, we defer the details and more results in the supplementary material. One sees that these results are quite intuitive. We plan a comprehensive study with ﬁeld experts to systematically evaluate the extraction results. Table 3. Examples of extracted edges with high conﬁdence. ﬁlm/ProducedBy ) ﬁlm/Country ﬁlm/ProductionCompanies ) ﬁlm/Country person/Nationality ) person/Languages person/PlaceOfBirth ) person/Languages person/PlaceOfBirth ) person/Nationality person/PlaceLivedLocation ) person/Nationality 5. Conclusion DAG structure learning is a challenging problem that has long been pursued in the literature of graphical models. The difﬁculty, in a large part, is owing to the NP-hardness incurred in the combinatorial formulation. Zheng et al. (2018) propose an equivalent continuous constraint that opens the opportunity of using well developed continuous optimization techniques for solving the problem. In this context, we explore the power of neural networks as functional approximators and develop a deep generative model to capture the complex data distribution, aiming at better recovering the underlying DAG with a different design of the objective function. In particular, we employ the machinery of variational autoencoders and parameterize them with new graph neural network architectures. The proposed method handles not only data generated by parametric models beyond linear, but also variables in general forms, including scalar/vector values and continuous/discrete types. We have performed extensive experiments on synthetic, benchmark, and application data and demonstrated the practical competitiveness of the proposal.',\n",
       " ' References Bertsekas, D. P. Nonlinear Programming . Athena Scientiﬁc, 2nd edition, 1999. Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y . Spectral networks and locally connected networks on graphs. In ICLR , 2014. Chen, E. Y .-J., Shen, Y ., Choi, A., and Darwiche, A. Learning Bayesian networks with ancestral constraints. In NIPS , 2016. Chen, J., Ma, T., and Xiao, C. FastGCN: Fast learning with graph convolutional networks via importance sampling. InICLR , 2018. Chickering, D. M. Optimal structure identiﬁcation with greedy search. Journal of Machine Learning Research , 2002. Chickering, D. M., Heckerman, D., and Meek, C. Largesample learning of Bayesian networks is NP-hard. Journal of Machine Learning Research , 5:1287–1330, 2004. Chow, C. and Liu, C. Approximating discrete probability distributions with dependence trees. IEEE transactions on Information Theory , 14(3):462–467, 1968. Cussens, J. Bayesian network learning with cutting planes. InUAI, 2011. Cussens, J., Haws, D., and Studen `y, M. Polyhedral aspects of score equivalence in Bayesian network structure learning. Mathematical Programming , pp. 1–40, 2016. Defferrard, M., Bresson, X., and Vandergheynst, P. Convolutional neural networks on graphs with fast localized spectral ﬁltering. In NIPS , 2016. Eaton, D. and Murphy, K. Bayesian structure learning using dynamic programming and MCMC. arXiv preprint arXiv:1206.5247 , 2012. Friedman, N. and Koller, D. Being Bayesian about network structure. a Bayesian approach to structure discovery in Bayesian networks. Machine learning , 50(1-2):95–125, 2003. Friedman, N., Goldszmidt, M., and Wyner, A. Data analysis with Bayesian networks: A bootstrap approach. In UAI, 1999. Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. In ICML , 2017. Grzegorczyk, M. and Husmeier, D. Improving the structure MCMC sampler for Bayesian networks by introducing a new edge reversal move. Machine Learning , 71(2-3):265, 2008.Gmez, J., Mateo, J., and Puerta, J. Learning Bayesian networks by hill climbing: efﬁcient methods based on progressive restriction of the neighborhood. Data Mining and Knowledge Discovery , 22(1-2):106–148, 2011. Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. In NIPS , 2017. He, R., Tian, J., and Wu, H. Structure learning in Bayesian networks of a moderate size by efﬁcient sampling. Journal of Machine Learning Research , 17(1):3483–3536, 2016. Heckerman, D., Geiger, D., and Chickering, D. M. Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning , 20(3):197–243, 1995. Jaakkola, T., Sontag, D., Globerson, A., and Meila, M. Learning Bayesian network structure using LP relaxations. 2010. Kalainathan, D., Goudet, O., Guyon, I., Lopez-Paz, D., and Sebag, M. SAM: Structural agnostic model, causal discovery and penalized adversarial learning. arXiv preprint arXiv:1803.04929 , 2018. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In ICLR , 2015. Kingma, D. P. and Welling, M. Auto-encoding variational Bayes. In ICLR , 2014. Kipf, T. N. and Welling, M. Semi-supervised classiﬁcation with graph convolutional networks. In ICLR , 2017. Koivisto, M. and Sood, K. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research , 5:549–573, 2004. Li, Y ., Tarlow, D., Brockschmidt, M., and Zemel, R. Gated graph sequence neural networks. In ICLR , 2016. Madigan, D., York, J., and Allard, D. Bayesian graphical models for discrete data. International Statistical Review/Revue Internationale de Statistique , pp. 215–232, 1995. Mohammadi, A., Wit, E. C., et al. Bayesian structure learning in sparse Gaussian graphical models. Bayesian Analysis, 10(1):109–138, 2015. Mohan, K., Chung, M., Han, S., Witten, D., Lee, S.-I., and Fazel, M. Structured learning of Gaussian graphical models. In NIPS , 2012. Mooij, J. M., Magliacane, S., and Claassen, T. Joint causal inference from multiple contexts. arXiv preprint arXiv:1611.10351 , 2016.',\n",
       " ' Nie, S., Mau ´a, D. D., De Campos, C. P., and Ji, Q. Advances in learning Bayesian networks of bounded treewidth. In NIPS , 2014. Niinimaki, T., Parviainen, P., and Koivisto, M. Partial order MCMC for structure discovery in Bayesian networks. arXiv preprint arXiv:1202.3753 , 2012. Niinim ¨aki, T. M. and Koivisto, M. Annealed importance sampling for structure learning in Bayesian networks. In IJCAI , 2013. Ott, S., Imoto, S., and Miyano, S. Finding optimal models for small gene networks. In Paciﬁc symposium on biocomputing , 2004. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. Automatic differentiation in PyTorch. 2017. Pearl, J. Probabilistic reasoning in intelligent systems: networks of plausible inference . Morgan Kaufmann Publishers, Inc., 2 edition, 1988. Pearl, J. Causality: models, reasoning, and inference. Econometric Theory , 19(46):675–685, 2003. Ramsey, J., Glymour, M., Sanchez-Romero, R., and Glymour, C. A million variables and more: the fast greedy equivalence search algorithm for learning highdimensional graphical causal models, with an application to functional magnetic resonance images. International Journal of Data Science and Analytics , 3(2):121–129, 2017. Sachs, K., Perez, O., Peer, D., Lauffenburger, D. A., and Nolan, G. P. Causal protein-signaling networks derived from multiparameter single-cell data. Science , 308(5721): 523–529, 2005. Scanagatta, M., de Campos, C. P., Corani, G., and Zaffalon, M. Learning Bayesian networks with thousands of variables. In NIPS , 2015. Silander, T. and Myllymaki, P. A simple approach for ﬁnding the globally optimal Bayesian network structure. In UAI, 2006. Singh, A. P. and Moore, A. W. Finding optimal Bayesian networks by dynamic programming. Technical report, Carnegie Mellon University, 2005. Spirtes, P., Meek, C., and Richardson, T. Causal inference in the presence of latent variables and selection bias. In UAI, 1995. Spirtes, P., Glymour, C. N., and Scheines, R. Computation, Causation, and Discovery . AAAI Press, 1999.Spirtes, P., Glymour, C., Scheines, R., Kauffman, S., Aimale, V ., and Wimberly, F. Constructing Bayesian network models of gene expression networks from microarray data, 2000a. Spirtes, P., Glymour, C. N., Scheines, R., Heckerman, D., Meek, C., Cooper, G., and Richardson, T. Causation, prediction, and search . MIT press, 2000b. Toutanova, K., Chen, D., Pantel, P., Poon, H., Choudhury, P., and Gamon, M. Representing text for joint embedding of text and knowledge bases. In EMNLP , 2015. Tsamardinos, I., Aliferis, C. F., and Statnikov, A. Time and sample efﬁcient discovery of markov blankets and direct causal relations. In SIGKDD , 2003. Tsamardinos, I., Brown, L., and Aliferis, C. The maxmin hill-climbing Bayesian network structure learning algorithm. Machine Learning , 65(1):31–78, 2006. Veli˘ckovi ´c, P., Cucurull, G., Casanova, A., Romero, A., Li `o, P., and Bengio, Y . Graph attention networks. In ICLR , 2018. Yuan, C. and Malone, B. Learning optimal Bayesian networks: A shortest path perspective. 48:23–65, 2013. Zhang, J. On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias. Artiﬁcial Intelligence , 172(16-17): 1873–1896, 2008. Zheng, X., Aragam, B., Ravikumar, P., and Xing, E. P. DAGs with NO TEARS: Continuous optimization for structure learning. In NIPS , 2018.',\n",
       " ' A. Proofs Proof of Theorem 1. LetB=A\\x0eA. Clearly,Bis nonnegative. The binomial expansion reads (I+\\x0bB)m=I+mX k=1\\x12m k\\x13 \\x0bkBk: It is known that there is a cycle of length kif and only if tr(Bk)>0whenB\\x150. Because if there is a cycle then there is a cycle of length at most m, we conclude that there is no cycle if and only if tr[(I+\\x0bB)m] = tr(I) =m. Proof of Theorem 2. Write (1 +\\x0bj\\x15j)m=\\x12 1 +cj\\x15j m\\x13m : For givencandj\\x15j, the right-hand side of the equality is a function of m. This function monotonically increases for positive mand has a limit ecj\\x15j. Hence, for any ﬁnite m> 0,(1 +\\x0bj\\x15j)m\\x14ecj\\x15j. B. Structure Learning over KB Relations We construct the data set from triples in FB15K-237 (Toutanova et al., 2015), which is a subset of FreeBase with approximately 15k entities and 237 relations. Each sample corresponds to an entity and each variable corresponds to a relation in this knowledge base. Each sample has on average 7.36 relations (i.e. 7.36 non-zero entries in each row). Table 4 gives additional examples learned by our model with highest conﬁdence scores. For each target relation on the right-hand side, we show the highest ranked relations within the same domain (i.e. the contents in the ﬁeld before “/” such as “ﬁlm” and “tvProgram”). On the left-hand side, we omit the relations that are common to the associated entity types, e.g. “profession” and “gender” to persons and “genre” to ﬁlms, because almost all entities with these types will contain such a relation.',\n",
       " ' Table 4. (Continued from Table 3) Examples of extracted edges with high conﬁdence. The dot \\x01appearing in R1:R2means that the sample entity is connected to a virtual node (i.e. compound value types in FreeBase) via relation R1, followed by a relation R2to a real entity. ﬁlm/ProducedBy ) ﬁlm/Country ﬁlm/ProductionCompanies ) ﬁlm/Country tvProgram/CountryOfOriginal ) tvProgram/Language tvProgram/RegularCast.regularTv/AppearanceActor ) tvProgram/Language person/Nationality ) person/Languages person/PlaceOfBirth ) person/Languages person/PlaceOfBirth ) person/Nationality person/PlaceLivedLocation ) person/Nationality organization/Headquarters.mailingAddress/Citytown ) organization/PlaceFounded organization/Headquarters.mailingAddress/StateProvince ) organization/PlaceFounded']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
